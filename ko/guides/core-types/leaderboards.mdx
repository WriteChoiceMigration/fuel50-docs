---
- title: 리더보드
- description: 평가 지표를 기반으로 다양한 모델 버전을 비교하고 순위를 매깁니다
---

Weave 사용하기*리더보드* 여러 지표에 걸쳐 여러 모델을 평가하고 비교하며 정확도, 생성 품질, 지연 시간 또는 사용자 정의 평가 로직을 측정합니다. 리더보드는 중앙 위치에서 모델 성능을 시각화하고, 시간 경과에 따른 변화를 추적하며, 팀 전체의 벤치마크에 맞출 수 있도록 도와줍니다.

리더보드는 다음과 같은 경우에 이상적입니다:

* 모델 성능 저하 추적
* 공유 평가 워크플로우 조정

## 리더보드 생성

리더보드는 [Weave UI](#ui) 또는 [프로그래밍 방식](#python)으로 생성할 수 있습니다.

### UI

Weave UI에서 직접 리더보드를 생성하고 사용자 지정하려면:

1. Weave UI에서 **Leaders** 섹션으로 이동합니다. 보이지 않는 경우 **More** → **Leaders**를 클릭합니다.
2. &#xNAN;**+ New Leaderboard**를 클릭합니다.
3. **Leaderboard Title** 필드에 설명적인 이름을 입력합니다(예: `summarization-benchmark-v1`).
4. 선택적으로 이 리더보드가 비교하는 내용을 설명하는 설명을 추가합니다.
5. [열 추가](#add-columns)하여 표시할 평가 및 지표를 정의합니다.
6. 레이아웃에 만족하면 리더보드를 저장하고 게시하여 다른 사람들과 공유합니다.

#### 열 추가

리더보드의 각 열은 특정 평가의 지표를 나타냅니다. 열을 구성하려면 다음을 지정합니다:

* **평가**: 드롭다운에서 평가 실행을 선택합니다(이전에 생성된 것이어야 함).
* **점수 산정자**: 해당 평가에 사용된 점수 산정 함수를 선택합니다(예: `jaccard_similarity`, `simple_accuracy`).
* **지표**: 표시할 요약 지표를 선택합니다(예: `mean`, `true_fraction` 등).

더 많은 열을 추가하려면 **Add Column**을 클릭합니다.

열을 편집하려면 오른쪽에 있는 세 개의 점 메뉴(`⋯`)를 클릭합니다. 다음과 같은 작업을 할 수 있습니다:

* **Move before / after** – 열 순서 변경
* **Duplicate** – 열 정의 복사
* **Delete** – 열 제거
* **Sort ascending** – 리더보드의 기본 정렬 설정(다시 클릭하면 내림차순으로 전환)

### Python

<Tip>
  완전하고 실행 가능한 코드 샘플을 찾고 계신가요? [End-to-end Python example](#end-to-end-python-example)을 참조하세요.
</Tip>

리더보드를 생성하고 게시하려면:

1. 테스트 데이터셋을 정의합니다. 내장된 [`Dataset`](datasets.mdx)을 사용하거나 입력 및 대상 목록을 수동으로 정의할 수 있습니다:

   ```python
   dataset = [
       {"input": "...", "target": "..."},
       ...
   ]
   ```

2. 하나 이상의 [scorers](../evaluation/scorers.mdx)를 정의합니다:

   ```python
   @weave.op
   def jaccard_similarity(target: str, output: str) -> float:
       ...
   ```

3. [`Evaluation`](../core-types/evaluations.mdx)를 생성합니다:

   ```python
   evaluation = weave.Evaluation(
       name="My Eval",
       dataset=dataset,
       scorers=[jaccard_similarity],
   )
   ```

4. 평가할 모델을 정의합니다:

   ```python
   @weave.op
   def my_model(input: str) -> str:
       ...
   ```

5. 평가를 실행합니다:

   ```python
    async def run_all():
        await evaluation.evaluate(model_vanilla)
        await evaluation.evaluate(model_humanlike)
        await evaluation.evaluate(model_messy)

   asyncio.run(run_all())
   ```

6. 리더보드를 생성합니다:

   ```python
   spec = leaderboard.Leaderboard(
       name="My Leaderboard",
       description="Evaluating models on X task",
       columns=[
           leaderboard.LeaderboardColumn(
               evaluation_object_ref=get_ref(evaluation).uri(),
               scorer_name="jaccard_similarity",
               summary_metric_path="mean",
           )
       ]
   )
   ```

7. 리더보드를 게시합니다.

   ```python
   weave.publish(spec)
   ```

8. 결과를 검색합니다:

   ```python
   results = leaderboard.get_leaderboard_results(spec, client)
   print(results)
   ```

## End-to-End Python 예제

다음 예제는 Weave Evaluations를 사용하여 리더보드를 생성하고 공유 데이터셋에서 세 가지 요약 모델을 사용자 정의 지표로 비교합니다. 작은 벤치마크를 생성하고, 각 모델을 평가하고, [Jaccard similarity](https://www.learndatasci.com/glossary/jaccard-similarity/)로 각 모델의 점수를 매기고, 결과를 Weave 리더보드에 게시합니다.

```python
import weave
from weave.flow import leaderboard
from weave.trace.ref_util import get_ref
import asyncio

client = weave.init("leaderboard-demo")

dataset = [
    {
        "input": "Weave is a tool for building interactive LLM apps. It offers observability, trace inspection, and versioning.",
        "target": "Weave helps developers build and observe LLM applications."
    },
    {
        "input": "The OpenAI GPT-4o model can process text, audio, and vision inputs, making it a multimodal powerhouse.",
        "target": "GPT-4o is a multimodal model for text, audio, and images."
    },
    {
        "input": "The W&B team recently added native support for agents and evaluations in Weave.",
        "target": "W&B added agents and evals to Weave."
    }
]

@weave.op
def jaccard_similarity(target: str, output: str) -> float:
    target_tokens = set(target.lower().split())
    output_tokens = set(output.lower().split())
    intersection = len(target_tokens & output_tokens)
    union = len(target_tokens | output_tokens)
    return intersection / union if union else 0.0

evaluation = weave.Evaluation(
    name="Summarization Quality",
    dataset=dataset,
    scorers=[jaccard_similarity],
)

@weave.op
def model_vanilla(input: str) -> str:
    return input[:50]

@weave.op
def model_humanlike(input: str) -> str:
    if "Weave" in input:
        return "Weave helps developers build and observe LLM applications."
    elif "GPT-4o" in input:
        return "GPT-4o supports text, audio, and vision input."
    else:
        return "W&B added agent support to Weave."

@weave.op
def model_messy(input: str) -> str:
    return "Summarizer summarize models model input text LLMs."

async def run_all():
    await evaluation.evaluate(model_vanilla)
    await evaluation.evaluate(model_humanlike)
    await evaluation.evaluate(model_messy)

asyncio.run(run_all())

spec = leaderboard.Leaderboard(
    name="Summarization Model Comparison",
    description="Evaluate summarizer models using Jaccard similarity on 3 short samples.",
    columns=[
        leaderboard.LeaderboardColumn(
            evaluation_object_ref=get_ref(evaluation).uri(),
            scorer_name="jaccard_similarity",
            summary_metric_path="mean",
        )
    ]
)

weave.publish(spec)

results = leaderboard.get_leaderboard_results(spec, client)
print(results)
```

### 리더보드 보기 및 해석

스크립트 실행이 완료된 후 리더보드를 확인합니다:

1. **Weave UI**에서 **Leaders** 탭으로 이동합니다. 보이지 않는 경우 **More**를 클릭한 다음 **Leaders**를 선택합니다.
2. 리더보드 이름을 클릭합니다—예: `Summarization Model Comparison`.

리더보드 테이블에서 각 행은 주어진 모델(`model_humanlike`, `model_vanilla`, `model_messy`)을 나타냅니다. `mean` 열은 모델의 출력과 참조 요약 간의 평균 Jaccard 유사도를 보여줍니다.

![A leaderboard in the Weave UI](imgs/leaderboard-example.png)

이 예제의 경우:

* `model_humanlike`이 약 46%의 중복으로 가장 좋은 성능을 보입니다.
* `model_vanilla`(단순 절단)는 약 21%를 얻습니다.
* `model_messy` 의도적으로 나쁜 모델은 약 2%의 점수를 받습니다.
