---
- title: リーダーボード
- description: 評価メトリクスに基づいて異なるモデルバージョンを比較・ランク付け
---

Weave を使用する*リーダーボード*を使用して、複数のモデルを複数のメトリクスで評価・比較し、精度、生成品質、レイテンシー、またはカスタム評価ロジックを測定します。リーダーボードは、モデルのパフォーマンスを一元的に可視化し、時間の経過に伴う変化を追跡し、チーム全体のベンチマークを調整するのに役立ちます。

リーダーボードは以下に最適です：

* モデルのパフォーマンス低下の追跡
* 共有評価ワークフローの調整

## リーダーボードの作成

リーダーボードは[Weave UI](#ui)または[プログラムで](#python)作成できます。

### UI

Weave UIで直接リーダーボードを作成・カスタマイズするには：

1. Weave UIで、**Leaders**セクションに移動します。表示されていない場合は、**More**→**Leaders**をクリックします。
2. &#xNAN;**+ New Leaderboard**をクリックします。
3. **Leaderboard Title**フィールドに、わかりやすい名前（例：`summarization-benchmark-v1`）を入力します。
4. 必要に応じて、このリーダーボードが何を比較するのかを説明する説明文を追加します。
5. [列を追加](#add-columns)して、表示する評価とメトリクスを定義します。
6. レイアウトに満足したら、リーダーボードを保存して公開し、他のユーザーと共有します。

#### 列の追加

リーダーボードの各列は、特定の評価からのメトリクスを表します。列を設定するには、以下を指定します：

* **Evaluation**：ドロップダウンから評価実行を選択します（事前に作成されている必要があります）。
* **Scorer**：その評価で使用されるスコアリング関数を選択します（例：`jaccard_similarity`、`simple_accuracy`）。
* **Metric**：表示する要約メトリクスを選択します（例：`mean`、`true_fraction`など）。

列を追加するには、**Add Column**をクリックします。

列を編集するには、右側の3点メニュー（`⋯`）をクリックします。以下の操作が可能です：

* **Move before / after**– 列の順序を変更
* **Duplicate**– 列の定義をコピー
* **Delete**– 列を削除
* **Sort ascending**– リーダーボードのデフォルトソートを設定（再度クリックすると降順に切り替わります）

### Python

<Tip>
  完全な実行可能なコードサンプルをお探しですか？[エンドツーエンドのPython例](#end-to-end-python-example)をご覧ください。
</Tip>

リーダーボードを作成して公開するには：

1. テストデータセットを定義します。組み込みの[`Dataset`](datasets.mdx)を使用するか、入力とターゲットのリストを手動で定義できます：

   ```python
   dataset = [
       {"input": "...", "target": "..."},
       ...
   ]
   ```

2. 1つ以上の[scorers](../evaluation/scorers.mdx)を定義します：

   ```python
   @weave.op
   def jaccard_similarity(target: str, output: str) -> float:
       ...
   ```

3. [`Evaluation`](../core-types/evaluations.mdx)を作成します：

   ```python
   evaluation = weave.Evaluation(
       name="My Eval",
       dataset=dataset,
       scorers=[jaccard_similarity],
   )
   ```

4. 評価するモデルを定義します：

   ```python
   @weave.op
   def my_model(input: str) -> str:
       ...
   ```

5. 評価を実行します：

   ```python
    async def run_all():
        await evaluation.evaluate(model_vanilla)
        await evaluation.evaluate(model_humanlike)
        await evaluation.evaluate(model_messy)

   asyncio.run(run_all())
   ```

6. リーダーボードを作成します：

   ```python
   spec = leaderboard.Leaderboard(
       name="My Leaderboard",
       description="Evaluating models on X task",
       columns=[
           leaderboard.LeaderboardColumn(
               evaluation_object_ref=get_ref(evaluation).uri(),
               scorer_name="jaccard_similarity",
               summary_metric_path="mean",
           )
       ]
   )
   ```

7. リーダーボードを公開します。

   ```python
   weave.publish(spec)
   ```

8. 結果を取得します：

   ```python
   results = leaderboard.get_leaderboard_results(spec, client)
   print(results)
   ```

## エンドツーエンドのPython例

以下の例では、Weave Evaluationsを使用して、共有データセット上で3つの要約モデルをカスタムメトリクスで比較するリーダーボードを作成します。小規模なベンチマークを作成し、各モデルを評価し、[Jaccard similarity](https://www.learndatasci.com/glossary/jaccard-similarity/)で各モデルをスコアリングし、結果をWeaveリーダーボードに公開します。

```python
import weave
from weave.flow import leaderboard
from weave.trace.ref_util import get_ref
import asyncio

client = weave.init("leaderboard-demo")

dataset = [
    {
        "input": "Weave is a tool for building interactive LLM apps. It offers observability, trace inspection, and versioning.",
        "target": "Weave helps developers build and observe LLM applications."
    },
    {
        "input": "The OpenAI GPT-4o model can process text, audio, and vision inputs, making it a multimodal powerhouse.",
        "target": "GPT-4o is a multimodal model for text, audio, and images."
    },
    {
        "input": "The W&B team recently added native support for agents and evaluations in Weave.",
        "target": "W&B added agents and evals to Weave."
    }
]

@weave.op
def jaccard_similarity(target: str, output: str) -> float:
    target_tokens = set(target.lower().split())
    output_tokens = set(output.lower().split())
    intersection = len(target_tokens & output_tokens)
    union = len(target_tokens | output_tokens)
    return intersection / union if union else 0.0

evaluation = weave.Evaluation(
    name="Summarization Quality",
    dataset=dataset,
    scorers=[jaccard_similarity],
)

@weave.op
def model_vanilla(input: str) -> str:
    return input[:50]

@weave.op
def model_humanlike(input: str) -> str:
    if "Weave" in input:
        return "Weave helps developers build and observe LLM applications."
    elif "GPT-4o" in input:
        return "GPT-4o supports text, audio, and vision input."
    else:
        return "W&B added agent support to Weave."

@weave.op
def model_messy(input: str) -> str:
    return "Summarizer summarize models model input text LLMs."

async def run_all():
    await evaluation.evaluate(model_vanilla)
    await evaluation.evaluate(model_humanlike)
    await evaluation.evaluate(model_messy)

asyncio.run(run_all())

spec = leaderboard.Leaderboard(
    name="Summarization Model Comparison",
    description="Evaluate summarizer models using Jaccard similarity on 3 short samples.",
    columns=[
        leaderboard.LeaderboardColumn(
            evaluation_object_ref=get_ref(evaluation).uri(),
            scorer_name="jaccard_similarity",
            summary_metric_path="mean",
        )
    ]
)

weave.publish(spec)

results = leaderboard.get_leaderboard_results(spec, client)
print(results)
```

### リーダーボードの表示と解釈

スクリプトの実行が完了したら、リーダーボードを表示します：

1. **Weave UI**で、**Leaders**タブに移動します。表示されていない場合は、**More**をクリックし、**Leaders**を選択します。
2. リーダーボードの名前をクリックします（例：`Summarization Model Comparison`）。

リーダーボードテーブルでは、各行が特定のモデル（`model_humanlike`、`model_vanilla`、`model_messy`）を表します。`mean`列は、モデルの出力と参照要約間のJaccard類似度の平均を示しています。

![A leaderboard in the Weave UI](imgs/leaderboard-example.png)

この例では：

* `model_humanlike`が最も良いパフォーマンスを示し、約46%の重複があります。
* `model_vanilla`（単純な切り捨て）は約21%です。
* `model_messy`意図的に悪いモデルで、約2%のスコアです。
