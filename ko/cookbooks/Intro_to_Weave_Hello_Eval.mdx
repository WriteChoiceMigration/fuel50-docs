---
- title: 평가 소개
- description: W&B Weave를 사용한 평가 소개 방법 알아보기
---

<Note>
  이것은 대화형 노트북입니다. 로컬에서 실행하거나 아래 링크를 사용할 수 있습니다:

  * [Open in Google Colab](https://colab.research.google.com/github/wandb/weave/blob/master/docs/notebooks/Intro_to_Weave_Hello_Eval.ipynb)
  * [View source on GitHub](https://github.com/wandb/weave/blob/master/docs/notebooks/Intro_to_Weave_Hello_Eval.ipynb)
</Note>

## 🔑 전제 조건

Weave 평가를 실행하기 전에 다음 전제 조건을 완료하세요.

1. W\&B Weave SDK를 설치하고 [API key](https://wandb.ai/settings#api)로 로그인하세요.
2. OpenAI SDK를 설치하고 [API key](https://platform.openai.com/api-keys)로 로그인하세요.
3. W\&B 프로젝트를 초기화하세요.

```python
# Install dependancies and imports
!pip install wandb weave openai -q

import os
from getpass import getpass

from openai import OpenAI
from pydantic import BaseModel

import weave

# 🔑 Setup your API keys
# Running this cell will prompt you for your API key with `getpass` and will not echo to the terminal.
#####
print("---")
print(
    "You can find your Weights and Biases API key here: https://wandb.ai/settings#api"
)
os.environ["WANDB_API_KEY"] = getpass("Enter your Weights and Biases API key: ")
print("---")
print("You can generate your OpenAI API key here: https://platform.openai.com/api-keys")
os.environ["OPENAI_API_KEY"] = getpass("Enter your OpenAI API key: ")
print("---")
#####

# 🏠 Enter your W&B project name
weave_client = weave.init("MY_PROJECT_NAME")  # 🐝 Your W&B project name
```

## 🐝 첫 번째 평가 실행하기

다음 코드 샘플은 Weave의 `Model` 및 `Evaluation` API를 사용하여 LLM을 평가하는 방법을 보여줍니다. 먼저, `weave.Model`를 상속받아 Weave 모델을 정의하고, 모델 이름과 프롬프트 형식을 지정하며, `predict` 메서드를 `@weave.op`로 추적합니다. `predict` 메서드는 OpenAI에 프롬프트를 보내고 Pydantic 스키마(`FruitExtract`)를 사용하여 응답을 구조화된 출력으로 파싱합니다. 그런 다음, 입력 문장과 예상 대상으로 구성된 작은 평가 데이터셋을 만듭니다. 다음으로, 모델의 출력을 대상 레이블과 비교하는 사용자 정의 점수 함수(역시 `@weave.op`를 사용하여 추적)를 정의합니다. 마지막으로, 모든 것을 `weave.Evaluation`로 감싸고 데이터셋과 점수 매기기를 지정한 다음 `evaluate()`를 호출하여 평가 파이프라인을 비동기적으로 실행합니다.

```python
# 1. Construct a Weave model
class FruitExtract(BaseModel):
    fruit: str
    color: str
    flavor: str

class ExtractFruitsModel(weave.Model):
    model_name: str
    prompt_template: str

    @weave.op()
    def predict(self, sentence: str) -> dict:
        client = OpenAI()

        response = client.beta.chat.completions.parse(
            model=self.model_name,
            messages=[
                {
                    "role": "user",
                    "content": self.prompt_template.format(sentence=sentence),
                }
            ],
            response_format=FruitExtract,
        )
        result = response.choices[0].message.parsed
        return result

model = ExtractFruitsModel(
    name="gpt4o",
    model_name="gpt-4o",
    prompt_template='Extract fields ("fruit": <str>, "color": <str>, "flavor": <str>) as json, from the following text : {sentence}',
)

# 2. Collect some samples
sentences = [
    "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.",
    "Pounits are a bright green color and are more savory than sweet.",
    "Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.",
]
labels = [
    {"fruit": "neoskizzles", "color": "purple", "flavor": "candy"},
    {"fruit": "pounits", "color": "green", "flavor": "savory"},
    {"fruit": "glowls", "color": "orange", "flavor": "sour, bitter"},
]
examples = [
    {"id": "0", "sentence": sentences[0], "target": labels[0]},
    {"id": "1", "sentence": sentences[1], "target": labels[1]},
    {"id": "2", "sentence": sentences[2], "target": labels[2]},
]

# 3. Define a scoring function for your evaluation
@weave.op()
def fruit_name_score(target: dict, output: FruitExtract) -> dict:
    target_flavors = [f.strip().lower() for f in target["flavor"].split(",")]
    output_flavors = [f.strip().lower() for f in output.flavor.split(",")]
    # Check if any target flavor is present in the output flavors
    matches = any(tf in of for tf in target_flavors for of in output_flavors)
    return {"correct": matches}

# 4. Run your evaluation
evaluation = weave.Evaluation(
    name="fruit_eval",
    dataset=examples,
    scorers=[fruit_name_score],
)
await evaluation.evaluate(model)
```

## 🚀 더 많은 예제를 찾고 계신가요?

* [evlauation pipeline end-to-end](https://weave-docs.wandb.ai/tutorial-eval)를 구축하는 방법을 알아보세요.
* [RAG application by building](https://weave-docs.wandb.ai/tutorial-rag)를 평가하는 방법을 알아보세요.
