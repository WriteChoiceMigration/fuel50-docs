---
- title: W&B Weave Self-Managed
- description: 自分のインフラストラクチャでWeaveをデプロイして管理する
---

<Warning>
  Weave on self-managed infrastructureは現在プライベートプレビュー中です。

  本番環境では、W\&Bは強く推奨しています [W\&B Dedicated Cloud](https://docs.wandb.ai/guides/hosting/hosting-options/dedicated_cloud)、そこではWeaveは一般提供されています。

  本番グレードの自己管理インスタンスをデプロイするには、`support@wandb.com`。
</Warning>

このガイドでは、自己管理環境でW\&B Weaveを実行するために必要なすべてのコンポーネントをデプロイする方法について説明します。

自己管理Weaveデプロイメントの重要なコンポーネントは[ClickHouseDB](https://clickhouse.com/)で、Weaveアプリケーションバックエンドはこれに依存しています。

デプロイメントプロセスでは完全に機能するClickHouseDBインスタンスがセットアップされますが、本番環境での信頼性と高可用性を確保するために追加の手順が必要な場合があります。

## 要件

* W\&B Platformがインストールされていること。詳細については、[Self-Managed Deployment Guide](https://docs.wandb.ai/guides/hosting/hosting-options/self-managed/)を参照してください。
* [Bitnami's ClickHouse Helm Chart](https://github.com/bitnami/charts/tree/main/bitnami/clickhouse)。
* ClickHouseストレージ用に事前設定されたS3バケット。設定の詳細については、[Provide S3 Credentials](#provide-s3-credentials)を参照してください。
* 以下の仕様を持つKubernetesクラスターノード：
  * CPU: 8 cores
  * RAM: 64 GB
  * Disk: 200GB+
* W\&BからのWeave対応ライセンス。ライセンスをリクエストするには、`support@wandb.com`にお問い合わせください。

<Tip>
  詳細なリファレンスアーキテクチャについては、[https://docs.wandb.ai/guides/hosting/self-managed/ref-arch/](https://docs.wandb.ai/guides/hosting/self-managed/ref-arch/#models-and-weave)を参照してください。
</Tip>

## 1. ClickHouseを設定する

このドキュメントのClickHouseデプロイメントでは、[Bitnami ClickHouse](https://bitnami.com/stack/clickhouse)パッケージを使用しています。

Bitnami Helmチャートは、基本的なClickHouse機能、特に[ClickHouse Keeper](https://clickhouse.com/docs/en/guides/sre/keeper/clickhouse-keeper)の使用に対して良いサポートを提供しています。

Clickhouseを設定するには、以下の手順を完了してください：

1. [Helmリポジトリを設定する](#configure-helm-repository)
2. [Helm設定を作成する](#create-helm-configuration)
3. [S3認証情報を提供する](#provide-s3-credentials)

### Helmリポジトリを設定する

1. Bitnami Helmリポジトリを追加します：

   `helm repo add bitnami https://charts.bitnami.com/bitnami`

2. リポジトリを更新します：

   `helm repo update`

### Helm設定を作成する

Helm設定の最も重要な部分は、XML形式で提供されるClickHouse設定です。以下は`values.yaml`ファイルの例で、ニーズに合わせてカスタマイズ可能なパラメータが含まれています。
設定プロセスを簡単にするために、関連セクションにコメントを追加しています。コメントは`{/* COMMENT */}`の形式で記載されています。

以下のパラメータを変更してください：

* `clusterName`
* `auth.username`
* `auth.password`
* S3バケット関連の設定

W\&Bは`clusterName`の値を`values.yaml`内で`weave_cluster`に設定することを推奨しています。これはW\&B Weaveがデータベースマイグレーションを実行する際に期待されるクラスター名です。異なる名前を使用する必要がある場合は、[Setting `clusterName`](#setting-clustername)セクションで詳細を確認してください。

```yaml
## @param clusterName ClickHouse cluster name
clusterName: weave_cluster

## @param shards Number of ClickHouse shards to deploy
shards: 1

## @param replicaCount Number of ClickHouse replicas per shard to deploy
## if keeper enable, same as keeper count, keeper cluster by shards.
replicaCount: 3

persistence:
  enabled: true
  size: 30G # this size must be larger than cache size.

## ClickHouse resource requests and limits
resources:
  requests:
    cpu: 0.5
    memory: 500Mi
  limits:
    cpu: 3.0
    memory: 6Gi

## Authentication
auth:
  username: weave_admin
  password: "weave_123"
  existingSecret: ""
  existingSecretKey: ""

## @param logLevel Logging level
logLevel: information

## @section ClickHouse keeper configuration parameters
keeper:
  enabled: true

## @param extraEnvVars Array with extra environment variables to add to ClickHouse nodes
##
extraEnvVars:
  - name: S3_ENDPOINT
    value: "https://s3.us-east-1.amazonaws.com/bucketname/$(CLICKHOUSE_REPLICA_ID)"


## @param defaultConfigurationOverrides [string] Default configuration overrides (evaluated as a template)
defaultConfigurationOverrides: |
  <clickhouse>
    {/* Macros */}
    <macros>
      <shard from_env="CLICKHOUSE_SHARD_ID"></shard>
      <replica from_env="CLICKHOUSE_REPLICA_ID"></replica>
    </macros>
    {/* Log Level */}
    <logger>
      <level>{{ .Values.logLevel }}</level>
    </logger>
    {{- if or (ne (int .Values.shards) 1) (ne (int .Values.replicaCount) 1)}}
    <remote_servers>
      <{{ .Values.clusterName }}>
        {{- $shards := $.Values.shards | int }}
        {{- range $shard, $e := until $shards }}
        <shard>
          <internal_replication>true</internal_replication>
          {{- $replicas := $.Values.replicaCount | int }}
          {{- range $i, $_e := until $replicas }}
          <replica>
            <host>{{ printf "%s-shard%d-%d.%s.%s.svc.%s" (include "common.names.fullname" $ ) $shard $i (include "clickhouse.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
            <port>{{ $.Values.service.ports.tcp }}</port>
          </replica>
          {{- end }}
        </shard>
        {{- end }}
      </{{ .Values.clusterName }}>
    </remote_servers>
    {{- end }}
    {{- if .Values.keeper.enabled }}
    <keeper_server>
      <tcp_port>{{ $.Values.containerPorts.keeper }}</tcp_port>
      {{- if .Values.tls.enabled }}
      <tcp_port_secure>{{ $.Values.containerPorts.keeperSecure }}</tcp_port_secure>
      {{- end }}
      <server_id from_env="KEEPER_SERVER_ID"></server_id>
      <log_storage_path>/bitnami/clickhouse/keeper/coordination/log</log_storage_path>
      <snapshot_storage_path>/bitnami/clickhouse/keeper/coordination/snapshots</snapshot_storage_path>
      <coordination_settings>
        <operation_timeout_ms>10000</operation_timeout_ms>
        <session_timeout_ms>30000</session_timeout_ms>
        <raft_logs_level>trace</raft_logs_level>
      </coordination_settings>
      <raft_configuration>
        {{- $nodes := .Values.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <server>
          <id>{{ $node | int }}</id>
          <hostname from_env="{{ printf "KEEPER_NODE_%d" $node }}"></hostname>
          <port>{{ $.Values.service.ports.keeperInter }}</port>
        </server>
        {{- end }}
      </raft_configuration>
    </keeper_server>
    {{- end }}
    {{- if or .Values.keeper.enabled .Values.zookeeper.enabled .Values.externalZookeeper.servers }}
    <zookeeper>
      {{- if or .Values.keeper.enabled }}
      {{- $nodes := .Values.replicaCount | int }}
      {{- range $node, $e := until $nodes }}
      <node>
        <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
        <port>{{ $.Values.service.ports.keeper }}</port>
      </node>
      {{- end }}
      {{- else if .Values.zookeeper.enabled }}
      {{- $nodes := .Values.zookeeper.replicaCount | int }}
      {{- range $node, $e := until $nodes }}
      <node>
        <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
        <port>{{ $.Values.zookeeper.service.ports.client }}</port>
      </node>
      {{- end }}
      {{- else if .Values.externalZookeeper.servers }}
      {{- range $node :=.Values.externalZookeeper.servers }}
      <node>
        <host>{{ $node }}</host>
        <port>{{ $.Values.externalZookeeper.port }}</port>
      </node>
      {{- end }}
      {{- end }}
    </zookeeper>
    {{- end }}
    {{- if .Values.metrics.enabled }}
    <prometheus>
      <endpoint>/metrics</endpoint>
      <port from_env="CLICKHOUSE_METRICS_PORT"></port>
      <metrics>true</metrics>
      <events>true</events>
      <asynchronous_metrics>true</asynchronous_metrics>
    </prometheus>
    {{- end }}
    <listen_host>0.0.0.0</listen_host>
    <listen_host>::</listen_host>
    <listen_try>1</listen_try>
    <storage_configuration>
      <disks>
        <s3_disk>
          <type>s3</type>
          <endpoint from_env="S3_ENDPOINT"></endpoint>

          {/* AVOID USE CREDENTIALS CHECK THE RECOMMENDATION */}
          <access_key_id>xxx</access_key_id>
          <secret_access_key>xxx</secret_access_key>
          {/* AVOID USE CREDENTIALS CHECK THE RECOMMENDATION */}

         <metadata_path>/var/lib/clickhouse/disks/s3_disk/</metadata_path>
        </s3_disk>
        <s3_disk_cache>
  	      <type>cache</type>
          <disk>s3_disk</disk>
          <path>/var/lib/clickhouse/s3_disk_cache/cache/</path>
          {/* THE CACHE SIZE MUST BE LOWER THAN PERSISTENT VOLUME */}
          <max_size>20Gi</max_size>
        </s3_disk_cache>
      </disks>
      <policies>
        <s3_main>
          <volumes>
            <main>
              <disk>s3_disk_cache</disk>
            </main>
          </volumes>
        </s3_main>
      </policies>
    </storage_configuration>
    <merge_tree>
      <storage_policy>s3_main</storage_policy>
    </merge_tree>
  </clickhouse>

## @section Zookeeper subchart parameters
zookeeper:
  enabled: false
```

### S3エンドポイント設定

各ClickHouseレプリカがバケット内の自分のフォルダにデータを読み書きできるようにするため、バケットエンドポイントを環境変数として設定する必要があります。

```
extraEnvVars:
  - name: S3_ENDPOINT
    value: "https://s3.us-east-1.amazonaws.com/bucketname/$(CLICKHOUSE_REPLICA_ID)"
```

<Warning>
  バケットエンドポイント設定から`$(CLICKHOUSE_REPLICA_ID)`を削除しないでください。これにより、各ClickHouseレプリカがバケット内の自分のフォルダからデータを読み書きすることが保証されます。
</Warning>

### S3認証情報を提供する

S3バケットにアクセスするための認証情報は、設定をハードコーディングするか、ClickHouseが環境変数またはEC2インスタンスからデータを取得するようにすることで指定できます。

#### 設定をハードコーディングする

ストレージ設定に認証情報を直接含めます：

```plaintext
<type>s3</type>
<endpoint from_env="S3_ENDPOINT"></endpoint>
<access_key_id>xxx</access_key_id>
<secret_access_key>xxx</secret_access_key>
```

#### 環境変数またはEC2メタデータを使用する

認証情報をハードコーディングする代わりに、環境変数またはAmazon EC2インスタンスメタデータから動的に取得するようにClickHouseを設定できます。

```plaintext
<use_environment_credentials>true</use_environment_credentials>
```

詳細については[ClickHouse: Separation of Storage and Compute](https://clickhouse.com/docs/en/guides/separation-storage-compute)を参照してください。

## 2. ClickHouseをインストールしてデプロイする

リポジトリをセットアップし、`values.yaml`ファイルを準備したら、次のステップはClickHouseをインストールすることです。

```bash
helm install clickhouse bitnami/clickhouse -f values.yaml --version 8.0.10
```

<Warning>
  バージョン`8.0.10`を使用していることを確認してください。最新のチャートバージョン（`9.0.0`）は、このドキュメントで提案されている設定では動作しません。
</Warning>

## 3. ClickHouseデプロイメントを確認する

以下のコマンドを使用して、ClickHouseがデプロイされていることを確認します：

```bash
kubectl get pods
```

以下のポッドが表示されるはずです：

```bash
NAME                                 READY   STATUS    RESTARTS   AGE
clickhouse-shard0-0                  1/1     Running   0          9m59s
clickhouse-shard0-1                  1/1     Running   0          10m
clickhouse-shard0-2                  1/1     Running   0          10m
```

## 4. Weaveをデプロイする

Weaveは[W\&B Operator](https://docs.wandb.ai/guides/hosting/operator/#wb-kubernetes-operator)を通じて自動デプロイが可能です。W\&B Platformがインストールされている状態で、以下の手順を完了してください：

1. プラットフォームのデプロイに使用される[CR instance](https://docs.wandb.ai/guides/hosting/operator/#complete-example)を編集します。
2. Weave設定を追加します。

## 5. 情報を収集する

1. Weaveトレーシングを設定するために、Kubernetesサービスの詳細を使用します：

* **Endpoint**: `<release-name>-headless.<namespace>.svc.cluster.local`
  * Replace `<release-name>` with your Helm release name
  * Replace `<namespace>` with your `NAMESPACE`
  * Get the service details: `kubectl get svc -n <namespace>`
* **Username**: Set in the `values.yaml`
* **Password**: Set in the `values.yaml`

2. この情報を使用して、以下の設定を追加してW\&B Platform Custom Resource(CR)を更新します：

   ```yaml
   apiVersion: apps.wandb.com/v1
   kind: WeightsAndBiases
   metadata:
     labels:
       app.kubernetes.io/name: weightsandbiases
       app.kubernetes.io/instance: wandb
     name: wandb
     namespace: default
   spec:
     values:
       global:
       [...]
         clickhouse:
           host: <release-name>-headless.<namespace>.svc.cluster.local
           port: 8123
           password: <password>
           user: <username>
           database: wandb_weave
           # `replicated` must be set to `true` if replicating data across multiple nodes
           # This is in preview, use the env var `WF_CLICKHOUSE_REPLICATED`
           replicated: true

         weave-trace:
           enabled: true
       [...]
       weave-trace:
         install: true
         extraEnv:
           WF_CLICKHOUSE_REPLICATED: "true"
       [...]
   ```

<Warning>
  複数のレプリカを使用する場合（W\&Bは少なくとも3つのレプリカを推奨）、Weave Tracesに対して以下の環境変数が設定されていることを確認してください。

  ```
  extraEnv:
    WF_CLICKHOUSE_REPLICATED: "true"
  ```

  これは`replicated: true`と同じ効果があり、プレビュー中です。
</Warning>

3. Set the `clusterName` in `values.yaml` to `weave_cluster`. If it is not, the database migration will fail.

   または、異なるクラスター名を使用する場合は、以下の例に示すように、`WF_CLICKHOUSE_REPLICATED_CLUSTER`環境変数を`weave-trace.extraEnv`で選択した名前と一致するように設定します。

   ```yaml
   [...]
     clickhouse:
       host: <release-name>-headless.<namespace>.svc.cluster.local
       port: 8123
       password: <password>
       user: <username>
       database: wandb_weave
       # `replicated` must be set to `true` if replicating data across multiple nodes
       # This is in preview, use the env var `WF_CLICKHOUSE_REPLICATED`
       replicated: true

     weave-trace:
       enabled: true
   [...]
   weave-trace:
     install: true
     extraEnv:
       WF_CLICKHOUSE_REPLICATED: "true"
       WF_CLICKHOUSE_REPLICATED_CLUSTER: "different_cluster_name"
   [...]
   ```

   最終的な設定は以下の例のようになります：

   ```yaml
   apiVersion: apps.wandb.com/v1
   kind: WeightsAndBiases
   metadata:
     labels:
       app.kubernetes.io/name: weightsandbiases
       app.kubernetes.io/instance: wandb
     name: wandb
     namespace: default
   spec:
     values:
       global:
         license: eyJhbGnUzaHgyQjQyQWhEU3...ZieKQ2x5GGfw
         host: https://wandb.example.com

         bucket:
           name: abc-wandb-moving-pipefish
           provider: gcs

         mysql:
           database: wandb_local
           host: 10.218.0.2
           name: wandb_local
           password: 8wtX6cJHizAZvYScjDzZcUarK4zZGjpV
           port: 3306
           user: wandb

         clickhouse:
           host: <release-name>-headless.<namespace>.svc.cluster.local
           port: 8123
           password: <password>
           user: <username>
           database: wandb_weave
           # This option must be true if replicating data across multiple nodes
           replicated: true

         weave-trace:
           enabled: true

       ingress:
         annotations:
           ingress.gcp.kubernetes.io/pre-shared-cert: abc-wandb-cert-creative-puma
           kubernetes.io/ingress.class: gce
           kubernetes.io/ingress.global-static-ip-name: abc-wandb-operator-address

       weave-trace:
         install: true
         extraEnv:
           WF_CLICKHOUSE_REPLICATED: "true"
   ```

4. Custom Resource (CR)を準備したら、新しい設定を適用します：

   ```bash
   kubectl apply -f wandb.yaml
   ```

## 6. Weaveにアクセスする

デプロイメントが実行されたら、`host`オプションで設定されたW\&Bエンドポイントにアクセスすると、Weaveライセンスのステータスが有効として表示されるはずです。

![Weave](../../media/weave-self-managed/weave-org-dashboard.png)
