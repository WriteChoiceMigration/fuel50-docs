# MistralAI

<a target="_blank" href="https://colab.research.google.com/github/wandb/examples/blob/master/weave/docs/quickstart_mistral.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a>

Weaveは[MistralAI Python library](https://github.com/mistralai/client-python)を介して行われるLLM呼び出しを自動的に追跡およびログに記録します。

> 新しいMistral v1.0 SDKをサポートしています。移行ガイドは[here](https://github.com/mistralai/client-python/blob/main/MIGRATION.mdx)

## Traces

LLMアプリケーションのトレースを開発中も本番環境でも中央データベースに保存することが重要です。これらのトレースはデバッグに使用し、アプリケーションの改善に役立つデータセットとして活用します。

Weaveは自動的にトレースをキャプチャします [mistralai](https://github.com/mistralai/client-python)。ライブラリは通常通り使用でき、まず次のように呼び出します `weave.init()`:

```python
import weave
weave.init("cheese_recommender")

# then use mistralai library as usual
import os
from mistralai import Mistral

api_key = os.environ["MISTRAL_API_KEY"]
model = "mistral-large-latest"

client = Mistral(api_key=api_key)

messages = [
    {
        "role": "user",
        "content": "What is the best French cheese?",
    },
]

chat_response = client.chat.complete(
    model=model,
    messages=messages,
)
```

WeaveはMistralAIライブラリを通じて行われるすべてのLLM呼び出しを追跡し記録します。Weaveウェブインターフェースでトレースを確認できます。

[![mistral\_trace.png](imgs/mistral_trace.png)](https://wandb.ai/capecape/mistralai_project/weave/calls)

## 独自のopsでラップする

Weave opsは結果を*reproducible*にします。実験中にコードを自動的にバージョン管理し、入力と出力をキャプチャします。単に[`@weave.op()`](/ja/guides/tracking/ops)でデコレートされた関数を作成して[`mistralai.client.MistralClient.chat()`](https://docs.mistral.ai/capabilities/completion/)を呼び出すと、Weaveが入力と出力を追跡します。チーズレコメンダーでこれを行う方法を見てみましょう：

```python
# highlight-next-line
@weave.op()
def cheese_recommender(region:str, model:str) -> str:
    "Recommend the best cheese in a given region"
    
    messages = [
        {
            "role": "user",
            "content": f"What is the best cheese in {region}?",
        },
    ]

    chat_response = client.chat.complete(
        model=model,
        messages=messages,
    )
    return chat_response.choices[0].message.content

cheese_recommender(region="France", model="mistral-large-latest")
cheese_recommender(region="Spain", model="mistral-large-latest")
cheese_recommender(region="Netherlands", model="mistral-large-latest")
```

[![mistral\_ops.png](imgs/mistral_ops.png)](https://wandb.ai/capecape/mistralai_project/weave/calls)

## 作成する`Model`より簡単な実験のために

多くの要素がある場合、実験の整理は難しくなります。[`Model`](/ja/guides/core-types/models)クラスを使用することで、システムプロンプトや使用するモデルなど、アプリの実験的な詳細をキャプチャして整理できます。これにより、アプリのさまざまなイテレーションを整理して比較するのに役立ちます。

コードのバージョン管理と入出力のキャプチャに加えて、[`Model`](/ja/guides/core-types/models)はアプリケーションの動作を制御する構造化されたパラメータをキャプチャし、最適なパラメータを簡単に見つけることができます。Weave Modelsを`serve`、および[`Evaluation`](/ja/guides/core-types/evaluations)と一緒に使用することもできます。

以下の例では、`model`と`country`を実験できます。これらのいずれかを変更するたびに、新しい*version*の`CheeseRecommender`が得られます。

```python
import weave
from mistralai import Mistral

weave.init("mistralai_project")

class CheeseRecommender(weave.Model): # Change to `weave.Model`
    model: str
    temperature: float

    @weave.op()
    def predict(self, region:str) -> str: # Change to `predict`
        "Recommend the best cheese in a given region"
        
        client = Mistral(api_key=api_key)

        messages = [
            {
                "role": "user",
                "content": f"What is the best cheese in {region}?",
            },
        ]

        chat_response = client.chat.complete(
            model=model,
            messages=messages,
            temperature=self.temperature
        )
        return chat_response.choices[0].message.content

cheese_model = CheeseRecommender(
    model="mistral-medium-latest",
    temperature=0.0
    )
result = cheese_model.predict(region="France")
print(result)
```

[![mistral\_model.png](imgs/mistral_model.png)](https://wandb.ai/capecape/mistralai_project/weave/models)
