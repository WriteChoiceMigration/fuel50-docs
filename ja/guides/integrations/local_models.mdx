# ローカルモデル

多くの開発者は、LLama-3、Mixtral、Gemma、Phiなどのオープンソースモデルをダウンロードしてローカルで実行しています。これらのモデルをローカルで実行する方法はいくつかありますが、WeaveはそれらがOpenAI SDKの互換性をサポートしている限り、いくつかの方法をすぐに使えるようにサポートしています。

## ローカルモデル関数を`@weave.op()`

Weaveを任意のLLMと簡単に統合するには、`weave.init('<your-project-name>')`でWeaveを初期化し、LLMへの呼び出しを`weave.op()`でラップするだけです。詳細については、[tracing](/ja/guides/tracking/tracing)に関するガイドをご覧ください。

## ローカルモデルを使用するためにOpenAI SDKコードを更新する

OpenAI SDK互換性をサポートするすべてのフレームワークやサービスには、いくつかの小さな変更が必要です。

最も重要なのは、`base_url`の初期化中の`openai.OpenAI()`変更です。

```python
client = openai.OpenAI(
    base_url="http://localhost:1234",
)
```

ローカルモデルの場合、`api_key`は任意の文字列でかまいませんが、上書きする必要があります。そうしないと、OpenAIは環境変数からそれを使用しようとしてエラーを表示します。

## OpenAI SDKがサポートするローカルモデルランナー

以下は、Hugging Faceからモデルをダウンロードしてコンピュータで実行できるアプリのリストで、OpenAI SDK互換性をサポートしています。

1. Nomic [GPT4All](https://www.nomic.ai/gpt4all) - 設定のローカルサーバーによるサポート（[FAQ](https://docs.gpt4all.io/gpt4all_help/faq.html)）
2. [LMStudio](https://lmstudio.ai/) - ローカルサーバーOpenAI SDKサポート [docs](https://lmstudio.ai/docs/local-server)
3. [Ollama](https://ollama.com/) - [Experimental Support](https://github.com/ollama/ollama/blob/main/docs/openai.mdx) for OpenAI SDK
4. llama.cpp via [llama-cpp-python](https://llama-cpp-python.readthedocs.io/en/latest/server/) pythonパッケージ
5. [llamafile](https://github.com/Mozilla-Ocho/llamafile#other-example-llamafiles) - `http://localhost:8080/v1` Llamafile実行時に自動的にOpenAI SDKをサポート
