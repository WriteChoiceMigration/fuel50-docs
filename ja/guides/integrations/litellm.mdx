---
- title: LiteLLM
- description: LiteLLMを介して行われるLLM呼び出しを自動的に追跡およびログに記録する
---

<a target="_blank" href="https://colab.research.google.com/github/wandb/examples/blob/master/weave/docs/quickstart_litellm.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a>

Weaveは `weave.init()` が呼び出された後、LiteLLMを介して行われるLLM呼び出しを自動的に追跡およびログに記録します。

## トレース

開発中および本番環境の両方で、LLMアプリケーションのトレースを中央データベースに保存することが重要です。これらのトレースはデバッグに使用し、アプリケーションの改善に役立つデータセットとして活用します。

> **Note:** LiteLLMを使用する場合は、`import litellm` を使用してライブラリをインポートし、`litellm.completion` の代わりに `from litellm import completion` で完了関数を呼び出してください。これにより、すべての関数とパラメータが正しく参照されます。

WeaveはLiteLLMのトレースを自動的にキャプチャします。通常通りライブラリを使用できます。まず `weave.init()` を呼び出すことから始めます：

```python
import litellm
import weave

# highlight-next-line
weave.init("weave_litellm_integration")

openai_response = litellm.completion(
    model="gpt-3.5-turbo", 
    messages=[{"role": "user", "content": "Translate 'Hello, how are you?' to French"}],
    max_tokens=1024
)
print(openai_response.choices[0].message.content)

claude_response = litellm.completion(
    model="claude-3-5-sonnet-20240620", 
    messages=[{"role": "user", "content": "Translate 'Hello, how are you?' to French"}],
    max_tokens=1024
)
print(claude_response.choices[0].message.content)
```

これでWeaveはLiteLLMを通じて行われるすべてのLLM呼び出しを追跡およびログに記録します。Weaveウェブインターフェースでトレースを表示できます。

## 独自のオペレーションでラッピングする

Weaveオペレーションは、実験中にコードを自動的にバージョン管理することで結果を再現可能にし、入力と出力をキャプチャします。単に `@weave.op()` で装飾された関数を作成してLiteLLMの完了関数を呼び出すだけで、Weaveが入力と出力を追跡します。以下に例を示します：

```python
import litellm
import weave

# highlight-next-line
weave.init("weave_litellm_integration")

# highlight-next-line
@weave.op()
def translate(text: str, target_language: str, model: str) -> str:
    response = litellm.completion(
        model=model,
        messages=[{"role": "user", "content": f"Translate '{text}' to {target_language}"}],
        max_tokens=1024
    )
    return response.choices[0].message.content

print(translate("Hello, how are you?", "French", "gpt-3.5-turbo"))
print(translate("Hello, how are you?", "Spanish", "claude-3-5-sonnet-20240620"))
```

## `Model` を作成して実験を容易にする

多くの要素がある場合、実験の整理は困難です。`Model` クラスを使用することで、システムプロンプトや使用しているモデルなど、アプリの実験的な詳細をキャプチャして整理できます。これにより、アプリのさまざまな反復を整理して比較するのに役立ちます。

コードのバージョン管理と入出力のキャプチャに加えて、Modelsはアプリケーションの動作を制御する構造化されたパラメータをキャプチャし、どのパラメータが最適に機能したかを簡単に見つけることができます。Weave Modelsを `serve` や評価と一緒に使用することもできます。

以下の例では、異なるモデルと温度で実験できます：

```python
import litellm
import weave

# highlight-next-line
weave.init('weave_litellm_integration')

# highlight-next-line
class TranslatorModel(weave.Model):
    model: str
    temperature: float
  
    # highlight-next-line
    @weave.op()
    def predict(self, text: str, target_language: str):
        response = litellm.completion(
            model=self.model,
            messages=[
                {"role": "system", "content": f"You are a translator. Translate the given text to {target_language}."},
                {"role": "user", "content": text}
            ],
            max_tokens=1024,
            temperature=self.temperature
        )
        return response.choices[0].message.content

# Create instances with different models
gpt_translator = TranslatorModel(model="gpt-3.5-turbo", temperature=0.3)
claude_translator = TranslatorModel(model="claude-3-5-sonnet-20240620", temperature=0.1)

# Use different models for translation
english_text = "Hello, how are you today?"

print("GPT-3.5 Translation to French:")
print(gpt_translator.predict(english_text, "French"))

print("\nClaude-3.5 Sonnet Translation to Spanish:")
print(claude_translator.predict(english_text, "Spanish"))
```

## 関数呼び出し

LiteLLMは互換性のあるモデルの関数呼び出しをサポートしています。Weaveはこれらの関数呼び出しを自動的に追跡します。

```python
import litellm
import weave

# highlight-next-line
weave.init("weave_litellm_integration")

response = litellm.completion(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Translate 'Hello, how are you?' to French"}],
    functions=[
        {
            "name": "translate",
            "description": "Translate text to a specified language",
            "parameters": {
                "type": "object",
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "The text to translate",
                    },
                    "target_language": {
                        "type": "string",
                        "description": "The language to translate to",
                    }
                },
                "required": ["text", "target_language"],
            },
        },
    ],
)

print(response)
```

プロンプトで使用した関数を自動的にキャプチャし、バージョン管理します。

[![litellm\_gif.png](imgs/litellm_gif.gif)](https://wandb.ai/a-sh0ts/weave_litellm_integration/weave/calls)
