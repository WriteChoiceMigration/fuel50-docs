---
- title: 모델
- description: 데이터와 코드를 결합한 구조화된 모델로 애플리케이션 버전 관리
---

`Model`는 데이터(구성, 훈련된 모델 가중치 또는 기타 정보 포함 가능)와 모델 작동 방식을 정의하는 코드의 조합입니다. 이 API와 호환되도록 코드를 구성하면 애플리케이션을 버전 관리하는 구조화된 방법의 이점을 얻을 수 있어 실험을 더 체계적으로 추적할 수 있습니다.

<Tabs>
  <Tab title="Python">
    Weave에서 모델을 만들려면 다음이 필요합니다:

    * 에서 상속받는 클래스 `weave.Model`
    * 모든 매개변수에 대한 타입 정의
    * 타입이 지정된 `predict` 함수와 `@weave.op()` 데코레이터

    ```python
    from weave import Model
    import weave

    class YourModel(Model):
        attribute1: str
        attribute2: int

        @weave.op()
        def predict(self, input_data: str) -> dict:
            # Model logic goes here
            prediction = self.attribute1 + ' ' + input_data
            return {'pred': prediction}
    ```

    다음과 같이 평소처럼 모델을 호출할 수 있습니다:

    ```python
    import weave
    weave.init('intro-example')

    model = YourModel(attribute1='hello', attribute2=5)
    model.predict('world')
    ```

    이렇게 하면 `predict`를 호출할 때마다 모델 설정과 함께 입력 및 출력이 추적됩니다.

    ## 모델의 자동 버전 관리

    모델을 정의하는 매개변수나 코드를 변경하면 이러한 변경 사항이 기록되고 버전이 업데이트됩니다.
    이를 통해 서로 다른 모델 버전 간의 예측을 비교할 수 있습니다. 이를 사용하여 프롬프트를 반복하거나 최신 LLM을 시도하고 다양한 설정 간의 예측을 비교할 수 있습니다.

    예를 들어, 여기서 새 모델을 만듭니다:

    ```python
    import weave
    weave.init('intro-example')

    model = YourModel(attribute1='howdy', attribute2=10)
    model.predict('world')
    ```

    이를 호출한 후, UI에서 이 모델의 두 가지 버전이 있으며, 각각 다른 추적된 호출이 있는 것을 볼 수 있습니다.

    ## 모델 서빙

    모델을 서빙하려면 다음을 호출하여 FastAPI 서버를 쉽게 시작할 수 있습니다:

    ```bash
    weave serve <your model ref>
    ```

    추가 지침은 [serve](/ko/guides/tools/serve)를 참조하세요.

    ## 프로덕션 호출 추적

    프로덕션 호출을 구분하기 위해 UI나 API에서 쉽게 필터링할 수 있도록 예측에 추가 속성을 추가할 수 있습니다.

    ```python
    with weave.attributes({'env': 'production'}):
        model.predict('world')
    ```
  </Tab>

  <Tab title="TypeScript">
    ```plaintext
    This feature is not available in TypeScript yet.  Stay tuned!
    ```
  </Tab>
</Tabs>
