---
- title: 미디어 로깅
- description: Weave로 비디오, 이미지 및 오디오 로깅 및 표시하기
---

Weave는 비디오, 이미지 및 오디오의 로깅과 표시를 지원합니다.

## 비디오

Weave는 자동으로 [`moviepy`](https://zulko.github.io/moviepy/)를 사용하여 비디오를 로깅합니다. 이를 통해 추적된 함수에 비디오 입력과 출력을 전달할 수 있으며, Weave가 자동으로 비디오 데이터의 업로드와 저장을 처리합니다.

<Note>
  비디오 지원은 현재 Python에서만 사용 가능합니다.
</Note>

사용 정보는 [비디오 지원](../tracking/video)을 참조하세요.

## 이미지

로깅 유형: `PIL.Image.Image`.

<Warning>
  Base64로 인코딩된 이미지 문자열(예: `data:image/jpeg;base64,...`)은 기술적으로 지원되지만 권장되지 않습니다. 성능 문제를 일으킬 수 있으므로 절대적으로 필요한 경우(예: 특정 API와의 통합)에만 사용해야 합니다.
</Warning>

다음 예제는 OpenAI DALL-E API를 통해 생성된 이미지를 로깅하는 방법을 보여줍니다:

<Tabs>
  <Tab title="Python">
    ```python
    import weave
    from openai import OpenAI
    import requests
    from PIL import Image

    weave.init('image-example')
    client = OpenAI()

    @weave.op
    def generate_image(prompt: str) -> Image:
        response = client.images.generate(
            model="dall-e-3",
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1,
        )
        image_url = response.data[0].url
        image_response = requests.get(image_url, stream=True)
        image = Image.open(image_response.raw)

        # return a PIL.Image.Image object to be logged as an image
        return image

    generate_image("a cat with a pumpkin hat")
    ```
  </Tab>

  <Tab title="TypeScript">
    ```typescript
    import {OpenAI} from 'openai';
    import * as weave from 'weave';

    async function main() {
        const client = await weave.init('image-example');
        const openai = new OpenAI();

        const generateImage = weave.op(async (prompt: string) => {
            const response = await openai.images.generate({
                model: 'dall-e-3',
                prompt: prompt,
                size: '1024x1024',
                quality: 'standard',
                n: 1,
            });
            const imageUrl = response.data[0].url;
            const imgResponse = await fetch(imageUrl);
            const data = Buffer.from(await imgResponse.arrayBuffer());

            return weave.weaveImage({data});
        });

        generateImage('a cat with a pumpkin hat');
    }

    main();
    ```
  </Tab>
</Tabs>

이 이미지는 Weave에 로깅되어 UI에 자동으로 표시됩니다.

![Screenshot of pumpkin cat trace view](imgs/cat-pumpkin-trace.png)

### 로깅 전 큰 이미지 크기 조정

UI 렌더링 비용과 저장 공간 영향을 줄이기 위해 로깅 전에 이미지 크기를 조정하는 것이 도움이 될 수 있습니다. `postprocess_output`를 `@weave.op`에서 사용하여 이미지 크기를 조정할 수 있습니다.

```python
from dataclasses import dataclass
from typing import Any
from PIL import Image
import weave

weave.init('image-resize-example')

# Custom output type
@dataclass
class ImageResult:
    label: str
    image: Image.Image

# Resize helper
def resize_image(image: Image.Image, max_size=(512, 512)) -> Image.Image:
    image = image.copy()
    image.thumbnail(max_size, Image.ANTIALIAS)
    return image

# Postprocess output to resize image before logging
def postprocess_output(output: ImageResult) -> ImageResult:
    resized = resize_image(output.image)
    return ImageResult(label=output.label, image=resized)

@weave.op(postprocess_output=postprocess_output)
def generate_large_image() -> ImageResult:
    # Create an example image to process (e.g., 2000x2000 red square)
    img = Image.new("RGB", (2000, 2000), color="red")
    return ImageResult(label="big red square", image=img)

generate_large_image()
```

## 오디오

로깅 유형: `wave.Wave_read`.

다음 예제는 OpenAI의 음성 생성 API를 사용하여 오디오 파일을 로깅하는 방법을 보여줍니다.

<Tabs>
  <Tab title="Python">
    ```python
    import weave
    from openai import OpenAI
    import wave

    weave.init("audio-example")
    client = OpenAI()

    @weave.op
    def make_audio_file_streaming(text: str) -> wave.Wave_read:
        with client.audio.speech.with_streaming_response.create(
            model="tts-1",
            voice="alloy",
            input=text,
            response_format="wav",
        ) as res:
            res.stream_to_file("output.wav")

        # return a wave.Wave_read object to be logged as audio
        return wave.open("output.wav")

    make_audio_file_streaming("Hello, how are you?")
    ```
  </Tab>

  <Tab title="TypeScript">
    ```typescript
    import {OpenAI} from 'openai';
    import * as weave from 'weave';

    async function main() {
        await weave.init('audio-example');
        const openai = new OpenAI();

        const makeAudioFileStreaming = weave.op(async function audio(text: string) {
            const response = await openai.audio.speech.create({
                model: 'tts-1',
                voice: 'alloy',
                input: text,
                response_format: 'wav',
            });

            const chunks: Uint8Array[] = [];
            for await (const chunk of response.body) {
                chunks.push(chunk);
            }
            return weave.weaveAudio({data: Buffer.concat(chunks)});
        });

        await makeAudioFileStreaming('Hello, how are you?');
    }

    main();
    ```
  </Tab>
</Tabs>

이 오디오는 Weave에 로깅되어 오디오 플레이어와 함께 UI에 자동으로 표시됩니다. 오디오 플레이어에서 원시 오디오 파형을 보고 다운로드할 수 있습니다.

![Screenshot of audio trace view](imgs/audio-trace.png)

<Tip>
  우리의 [오디오 로깅](/ko/cookbooks/audio_with_weave) 또는 <a href="https://colab.research.google.com/github/wandb/weave/blob/master/docs/./notebooks/audio_with_weave.ipynb" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link button button--secondary button--med margin-right--sm notebook-cta-button"><div><img src="https://upload.wikimedia.org/wikipedia/commons/archive/d/d0/20221103151430%21Google_Colaboratory_SVG_Logo.svg" alt="Open In Colab" height="20px" /><div>Colab에서 열기</div></div></a> 쿡북을 시도해보세요. 이 쿡북에는 Weave와 통합된 실시간 오디오 API 기반 어시스턴트의 고급 예제도 포함되어 있습니다.
</Tip>
