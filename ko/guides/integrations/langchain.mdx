---
- title: LangChain
- description: Weave를 사용하여 LangChain Python 라이브러리를 통해 이루어진 모든 호출을 추적하고 로깅하세요
---

<a target="_blank" href="https://colab.research.google.com/github/wandb/examples/blob/master/weave/docs/quickstart_langchain.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a>

Weave는 [LangChain Python library](https://github.com/langchain-ai/langchain)를 통해 이루어진 모든 호출을 쉽게 추적하고 로깅할 수 있도록 설계되었습니다.

LLM으로 작업할 때 디버깅은 불가피합니다. 모델 호출이 실패하거나, 출력 형식이 잘못되거나, 중첩된 모델 호출이 혼란을 야기하는 경우 문제를 정확히 파악하는 것이 어려울 수 있습니다. LangChain 애플리케이션은 종종 여러 단계와 LLM 호출로 구성되어 있어 체인과 에이전트의 내부 작동 방식을 이해하는 것이 중요합니다.

Weave는 [LangChain](https://python.langchain.com/v0.2/docs/introduction/) 애플리케이션에 대한 추적을 자동으로 캡처하여 이 과정을 단순화합니다. 이를 통해 애플리케이션의 성능을 모니터링하고 분석할 수 있어 LLM 워크플로우를 더 쉽게 디버깅하고 최적화할 수 있습니다.

## 시작하기

시작하려면 스크립트 시작 부분에서 간단히 `weave.init()`를 호출하세요. weave.init()의 인수는 추적을 구성하는 데 도움이 되는 프로젝트 이름입니다.

```python
import weave
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# Initialize Weave with your project name
# highlight-next-line
weave.init("langchain_demo")

llm = ChatOpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

llm_chain = prompt | llm

output = llm_chain.invoke({"number": 2})

print(output)
```

## 호출 메타데이터 추적

LangChain 호출의 메타데이터를 추적하려면 [`weave.attributes`](https://weave-docs.wandb.ai/reference/python-sdk/weave/#function-attributes) 컨텍스트 관리자를 사용할 수 있습니다. 이 컨텍스트 관리자를 사용하면 체인이나 단일 요청과 같은 특정 코드 블록에 대한 사용자 정의 메타데이터를 설정할 수 있습니다.

```python
import weave
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# Initialize Weave with your project name
# highlight-next-line
weave.init("langchain_demo")

llm = ChatOpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

llm_chain = prompt | llm

# highlight-next-line
with weave.attributes({"my_awesome_attribute": "value"}):
    output = llm_chain.invoke()

print(output)
```

Weave는 LangChain 호출의 추적에 대해 메타데이터를 자동으로 추적합니다. 아래와 같이 Weave 웹 인터페이스에서 메타데이터를 볼 수 있습니다:

[![langchain\_attributes.png](imgs/langchain_attributes.png)](https://wandb.ai/parambharat/langchain_demo/weave/traces?cols=%7B%22attributes.weave.client_version%22%3Afalse%2C%22attributes.weave.os_name%22%3Afalse%2C%22attributes.weave.os_release%22%3Afalse%2C%22attributes.weave.os_version%22%3Afalse%2C%22attributes.weave.source%22%3Afalse%2C%22attributes.weave.sys_version%22%3Afalse%7D)

## 추적

개발 및 프로덕션 단계에서 LLM 애플리케이션의 추적을 중앙 데이터베이스에 저장하는 것이 중요합니다. 이러한 추적은 가치 있는 데이터셋을 제공하여 애플리케이션을 디버깅하고 개선하는 데 필수적입니다.

Weave는 LangChain 애플리케이션에 대한 추적을 자동으로 캡처합니다. 프롬프트 템플릿, 체인, LLM 호출, 도구 및 에이전트 단계를 포함하여 LangChain 라이브러리를 통해 이루어진 모든 호출을 추적하고 로깅합니다. Weave 웹 인터페이스에서 추적을 볼 수 있습니다.

[![langchain\_trace.png](imgs/langchain_trace.png)](https://wandb.ai/parambharat/langchain_demo/weave/calls)

## 수동으로 호출 추적하기

자동 추적 외에도 `WeaveTracer` 콜백이나 `weave_tracing_enabled` 컨텍스트 관리자를 사용하여 수동으로 호출을 추적할 수 있습니다. 이러한 방법은 LangChain 애플리케이션의 개별 부분에서 요청 콜백을 사용하는 것과 유사합니다.

**Note:** Weave는 기본적으로 Langchain Runnables를 추적하며 이는 `weave.init()`를 호출할 때 활성화됩니다. 환경 변수 `WEAVE_TRACE_LANGCHAIN`를 `"false"`로 설정하여 `weave.init()`를 호출하기 전에 이 동작을 비활성화할 수 있습니다. 이를 통해 애플리케이션의 특정 체인이나 개별 요청의 추적 동작을 제어할 수 있습니다.

### Using `WeaveTracer`

You can pass the `WeaveTracer` callback to individual LangChain components to trace specific requests.

```python
import os

os.environ["WEAVE_TRACE_LANGCHAIN"] = "false" # <- explicitly disable global tracing.

from weave.integrations.langchain import WeaveTracer
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import weave

# Initialize Weave with your project name
# highlight-next-line
weave.init("langchain_demo")  # <-- we don't enable tracing here because the env var is explicitly set to `false`

# highlight-next-line
weave_tracer = WeaveTracer()

# highlight-next-line
config = {"callbacks": [weave_tracer]}

llm = ChatOpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

llm_chain = prompt | llm

# highlight-next-line
output = llm_chain.invoke({"number": 2}, config=config) # <-- this enables tracing only for this chain invoke.

llm_chain.invoke({"number": 4})  # <-- this will not have tracing enabled for langchain calls but openai calls will still be traced
```

### Using `weave_tracing_enabled` Context Manager

Alternatively, you can use the `weave_tracing_enabled` context manager to enable tracing for specific blocks of code.

```python
import os

os.environ["WEAVE_TRACE_LANGCHAIN"] = "false" # <- explicitly disable global tracing.

from weave.integrations.langchain import weave_tracing_enabled
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import weave

# Initialize Weave with your project name
# highlight-next-line
weave.init("langchain_demo")  # <-- we don't enable tracing here because the env var is explicitly set to `false`

llm = ChatOpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

llm_chain = prompt | llm

# highlight-next-line
with weave_tracing_enabled():  # <-- this enables tracing only for this chain invoke.
    output = llm_chain.invoke({"number": 2})


llm_chain.invoke({"number": 4})  # <-- this will not have tracing enabled for langchain calls but openai calls will still be traced
```

## 구성

Upon calling `weave.init`, tracing is enabled by setting the environment variable `WEAVE_TRACE_LANGCHAIN`를 `"true"`로 설정하여 추적이 활성화됩니다. 이를 통해 Weave는 LangChain 애플리케이션에 대한 추적을 자동으로 캡처할 수 있습니다. 이 동작을 비활성화하려면 환경 변수를 `"false"`로 설정하세요.

## LangChain 콜백과의 관계

### 자동 로깅

The automatic logging provided by `weave.init()`는 LangChain 애플리케이션의 모든 구성 요소에 생성자 콜백을 전달하는 것과 유사합니다. 이는 프롬프트 템플릿, 체인, LLM 호출, 도구 및 에이전트 단계를 포함한 모든 상호 작용이 전체 애플리케이션에서 전역적으로 추적된다는 것을 의미합니다.

### 수동 로깅

수동 로깅 방법(`WeaveTracer`와 `weave_tracing_enabled`)은 LangChain 애플리케이션의 개별 부분에서 요청 콜백을 사용하는 것과 유사합니다. 이러한 방법은 애플리케이션의 어떤 부분이 추적되는지에 대한 더 세밀한 제어를 제공합니다:

* **생성자 콜백:** 전체 체인이나 구성 요소에 적용되어 모든 상호 작용을 일관되게 로깅합니다.
* **요청 콜백:**&#xD2B9;정 요청에 적용되어 특정 호출에 대한 상세한 추적을 가능하게 합니다.

Weave를 LangChain과 통합함으로써 LLM 애플리케이션의 포괄적인 로깅 및 모니터링을 보장하여 디버깅과 성능 최적화를 용이하게 할 수 있습니다.

더 자세한 정보는 다음을 참조하세요 [LangChain 문서](https://python.langchain.com/v0.2/docs/how_to/debugging/#tracing).

## 모델 및 평가

다양한 사용 사례에 대한 애플리케이션에서 LLM을 구성하고 평가하는 것은 프롬프트, 모델 구성, 추론 매개변수와 같은 여러 구성 요소가 있어 어렵습니다. [`weave.Model`](/ko/guides/core-types/models)를 사용하면 시스템 프롬프트나 사용하는 모델과 같은 실험 세부 정보를 캡처하고 구성하여 다양한 반복을 비교하기 쉽게 만들 수 있습니다.

다음 예제는 Langchain 체인을 `WeaveModel`로 래핑하는 방법을 보여줍니다:

```python
import json
import asyncio

import weave

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# Initialize Weave with your project name
# highlight-next-line
weave.init("langchain_demo")

# highlight-next-line
class ExtractFruitsModel(weave.Model):
    model_name: str
    prompt_template: str

# highlight-next-line
    @weave.op()
    async def predict(self, sentence: str) -> dict:
        llm = ChatOpenAI(model=self.model_name, temperature=0.0)
        prompt = PromptTemplate.from_template(self.prompt_template)

        llm_chain = prompt | llm
        response = llm_chain.invoke({"sentence": sentence})
        result = response.content

        if result is None:
            raise ValueError("No response from model")
        parsed = json.loads(result)
        return parsed

model = ExtractFruitsModel(
    model_name="gpt-3.5-turbo-1106",
    prompt_template='Extract fields ("fruit": <str>, "color": <str>, "flavor": <str>) from the following text, as json: {sentence}',
)
sentence = "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy."

prediction = asyncio.run(model.predict(sentence))

# if you're in a Jupyter Notebook, run:
# prediction = await model.predict(sentence)

print(prediction)
```

이 코드는 Weave UI에서 시각화할 수 있는 모델을 생성합니다:

[![langchain\_model.png](imgs/langchain_model.png)](https://wandb.ai/parambharat/langchain_demo/weave/object-versions?filter=%7B%22baseObjectClass%22%3A%22Model%22%7D\&peekPath=%2Fparambharat%2Flangchain_demo%2Fobjects%2FExtractFruitsModel%2Fversions%2FBeoL6WuCH8wgjy6HfmuBMyKzArETg1oAFpYaXZSq1hw%3F%26)

Weave Models를 `serve`, 및 [`Evaluations`](/ko/guides/core-types/evaluations)와 함께 사용할 수도 있습니다.

### 평가

평가는 모델의 성능을 측정하는 데 도움이 됩니다. [`weave.Evaluation`](/ko/guides/core-types/evaluations) 클래스를 사용하면 모델이 특정 작업이나 데이터셋에서 얼마나 잘 수행되는지 캡처하여 다양한 모델과 애플리케이션의 반복을 비교하기 쉽게 만들 수 있습니다. 다음 예제는 우리가 생성한 모델을 평가하는 방법을 보여줍니다:

```python

from weave.scorers import MultiTaskBinaryClassificationF1

sentences = [
    "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.",
    "Pounits are a bright green color and are more savory than sweet.",
    "Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.",
]
labels = [
    {"fruit": "neoskizzles", "color": "purple", "flavor": "candy"},
    {"fruit": "pounits", "color": "bright green", "flavor": "savory"},
    {"fruit": "glowls", "color": "pale orange", "flavor": "sour and bitter"},
]
examples = [
    {"id": "0", "sentence": sentences[0], "target": labels[0]},
    {"id": "1", "sentence": sentences[1], "target": labels[1]},
    {"id": "2", "sentence": sentences[2], "target": labels[2]},
]

@weave.op()
def fruit_name_score(target: dict, output: dict) -> dict:
    return {"correct": target["fruit"] == output["fruit"]}


evaluation = weave.Evaluation(
    dataset=examples,
    scorers=[
        MultiTaskBinaryClassificationF1(class_names=["fruit", "color", "flavor"]),
        fruit_name_score,
    ],
)
scores = asyncio.run(evaluation.evaluate(model)))
# if you're in a Jupyter Notebook, run:
# scores = await evaluation.evaluate(model)

print(scores)
```

이 코드는 Weave UI에서 시각화할 수 있는 평가 추적을 생성합니다:

[![langchain\_evaluation.png](imgs/langchain_eval.png)](https://wandb.ai/parambharat/langchain_demo/weave/calls?filter=%7B%22traceRootsOnly%22%3Atrue%7D\&peekPath=%2Fparambharat%2Flangchain_demo%2Fcalls%2F44c3f26c-d9d3-423e-b434-651ea5174be3)

Weave를 Langchain과 통합함으로써 LLM 애플리케이션의 포괄적인 로깅 및 모니터링을 보장하여 디버깅과 성능 최적화를 용이하게 할 수 있습니다.

## 알려진 문제

* **비동기 호출 추적** - Langchain의 `AsyncCallbackManager` 구현에 버그가 있어 비동기 호출이 올바른 순서로 추적되지 않습니다. 이를 수정하기 위해 [PR](https://github.com/langchain-ai/langchain/pull/23909)을 제출했습니다. 따라서 Langchain Runnables에서 `ainvoke`, `astream` 및 `abatch` 메서드를 사용할 때 추적에서 호출 순서가 정확하지 않을 수 있습니다.
