---
- title: FAQs
- description: Weave 추적에 관한 일반적인 질문에 대한 답변
---

다음 페이지는 Weave 추적에 관한 일반적인 질문에 대한 답변을 제공합니다.

## Weave는 함수에 대해 어떤 정보를 캡처합니까?

함수는 데코레이터를 통해 수동으로 또는 활성화된 통합의 일부로 자동으로 Weave [Op](/ko/guides/tracking/ops)로 지정될 수 있습니다. Op가 실행되면 Weave는 분석을 지원하기 위해 상세한 정보를 캡처합니다. Weave는 기본값과 다른 것을 원할 경우 로깅되는 내용을 세밀하게 제어할 수 있습니다. 구성 예는 아래를 참조하세요.

* **코드 캡처** - Weave는 Op의 소스 코드 표현을 캡처합니다. 여기에는 인라인 주석과 호출된 변수 값 또는 Op가 아닌 함수의 소스를 재귀적으로 캡처하는 것이 포함됩니다. 코드 캡처를 통해 변경 사항이 소스 제어 시스템에 저장되지 않았더라도 함수가 수행한 작업을 볼 수 있습니다. 코드 캡처는 Op 버전 관리의 일부로 사용되어 시간이 지남에 따라 코드 평가를 이해할 수 있게 합니다. 코드 캡처가 비활성화된 경우 해시 값이 대신 사용됩니다.

* **함수 이름, 입력 및 출력** - 함수 이름이 캡처되지만 [overridden](/ko/guides/tracking/tracing#call-display-name). JSON 기반 입력 및 출력 표현이 캡처됩니다. 입력의 경우, 값 외에도 인수 이름이 캡처됩니다. Weave를 사용하면 [customize the logging](/ko/guides/tracking/ops#customize-logged-inputs-and-outputs) 입력 및 출력의 - 추가/제거/수정할 내용을 지정하는 함수를 지정할 수 있습니다.

* **Op call hierarchy** - Op가 다른 Op 실행 컨텍스트 내에서 호출될 때, 중간에 비 Op 함수가 실행되는 경우에도 이 관계가 캡처됩니다. Op 호출 간의 이 관계는 "Trace tree"를 제공하는 데 사용됩니다.

* **Execution status and exceptions** - Weave는 함수가 실행 중인지, 완료되었는지 또는 오류가 발생했는지 추적합니다. 실행 중에 예외가 발생하면 오류 메시지와 스택 추적이 기록됩니다.

* **System information** - Weave는 클라이언트가 실행 중인 운영 체제에 대한 정보와 자세한 버전 정보를 캡처할 수 있습니다.

* **Client information** - Weave는 사용 중인 프로그래밍 언어 및 해당 언어와 Weave 클라이언트 라이브러리에 대한 자세한 버전 정보와 같은 Weave 클라이언트 자체에 대한 정보를 캡처할 수 있습니다.

* **Timing** - 실행 시작 및 종료 시간이 캡처되어 지연 시간 계산에도 사용됩니다.

* **Token usage** - 일부 [integrations](/guides/integrations/) LLM 토큰 사용량이 자동으로 기록될 수 있습니다.

* **User and run context** - 로깅은 W\&B 사용자 계정과 연결됩니다. 이는 wandb Run 컨텍스트와 함께 캡처됩니다.

* **Derived information** - Weave는 기록된 원시 정보에서 파생 정보를 계산할 수 있습니다. 예를 들어, 토큰 사용량과 사용된 모델에 대한 지식을 기반으로 비용 추정치가 계산될 수 있습니다. Weave는 또한 호출에 대한 일부 정보를 집계합니다.

* **Additional information you choose** - 선택적으로 [custom metadata with `weave.attributes`](/ko/guides/core-types/models#track-production-calls) 호출의 일부로 기록하거나 [feedback](/ko/guides/tracking/feedback#add-feedback-to-a-call) 호출에 첨부할 수 있습니다.

## How can I disable code capture?

Weave 클라이언트 초기화 중에 코드 캡처를 비활성화할 수 있습니다: `weave.init("entity/project", settings={"capture_code": False})`.
또한 [environment variable](/ko/guides/core-types/env-vars) `WEAVE_CAPTURE_CODE=false`.

## How can I disable system information capture?

Weave 클라이언트 초기화 중에 시스템 정보 캡처를 비활성화할 수 있습니다: `weave.init("entity/project", settings={"capture_system_info": False})`.

## How can I disable client information capture?

Weave 클라이언트 초기화 중에 클라이언트 정보 캡처를 비활성화할 수 있습니다: `weave.init("entity/project", settings={"capture_client_info": False})`.

## How do I render Python datetime values in the UI?

Python의 `datetime.datetime` (시간대 정보 포함)를 사용하고, `weave.publish(...)`를 사용하여 객체를 게시하세요. Weave는 이 유형을 인식하고 타임스탬프로 렌더링합니다.

## How do I render Markdown in the UI?

문자열을 `weave.Markdown(...)`로 감싼 후 저장하고, `weave.publish(...)`를 사용하여 저장하세요. Weave는 객체의 유형을 사용하여 렌더링을 결정하며, `weave.Markdown`는 알려진 UI 렌더러에 매핑됩니다. 값은 UI에서 형식이 지정된 Markdown 객체로 표시됩니다. 전체 코드 샘플은 [Viewing calls](./tracing.mdx#viewing-calls)를 참조하세요.

## Will Weave affect my function's execution speed?

Weave 로깅의 오버헤드는 일반적으로 LLM에 대한 호출에 비해 무시할 수 있는 수준입니다.
Op 실행 속도에 대한 Weave의 영향을 최소화하기 위해 네트워크 활동은 백그라운드 스레드에서 발생합니다.
프로그램이 종료될 때 남아있는 대기 중인 데이터가 기록되는 동안 일시 중지된 것처럼 보일 수 있습니다.

## How is Weave data ingestion calculated?

수집된 바이트는 수신, 처리 및 저장하는 바이트로 정의합니다. 여기에는 트레이스 메타데이터, LLM 입력/출력 및 Weave에 명시적으로 기록하는 기타 정보가 포함되지만 통신 오버헤드(예: HTTP 헤더) 또는 장기 저장소에 저장되지 않는 기타 데이터는 포함되지 않습니다. 바이트는 수신 및 저장되는 시점에 한 번만 "수집"된 것으로 계산됩니다.

## What is pairwise evaluation and how do I do it?

Weave [scoring](../evaluation/scorers.mdx) 모델을 [evaluation](../core-types/evaluations.mdx)할 때, 절대값 메트릭(예: 모델 A의 경우 `9/10`, 모델 B의 경우 `8/10`)은 일반적으로 상대적인 것(예: 모델 A가 모델 B보다 성능이 좋음)보다 할당하기 어렵습니다. *Pairwise evaluation*을 통해 두 모델의 출력을 상대적으로 순위를 매겨 비교할 수 있습니다. 이 접근 방식은 텍스트 생성, 요약 또는 질문 응답과 같은 주관적인 작업에 대해 어떤 모델이 더 나은 성능을 보이는지 결정하려는 경우에 특히 유용합니다. 쌍별 평가를 통해 특정 입력에 대해 어떤 모델이 가장 적합한지 보여주는 상대적 선호도 순위를 얻을 수 있습니다.

<Warning>
  이 접근 방식은 임시 해결책이며 향후 릴리스에서 변경될 수 있습니다. 쌍별 평가를 지원하기 위한 더 강력한 API를 적극적으로 개발 중입니다. 업데이트를 기대해 주세요!
</Warning>

다음 코드 샘플은 [class-based scorer](../evaluation/scorers.md#class-based-scorers)라는 `PreferenceScorer`를 만들어 Weave에서 쌍별 평가를 구현하는 방법을 보여줍니다. `PreferenceScorer`는 두 모델 `ModelA`와 `ModelB`를 비교하고 입력 텍스트의 명시적 힌트를 기반으로 모델 출력의 상대적 점수를 반환합니다.

```python
from weave import Model, Evaluation, Scorer, Dataset
from weave.flow.model import ApplyModelError, apply_model_async

class ModelA(Model):
    @weave.op
    def predict(self, input_text: str):
        if "Prefer model A" in input_text:
            return {"response": "This is a great answer from Model A"}
        return {"response": "Meh, whatever"}

class ModelB(Model):
    @weave.op
    def predict(self, input_text: str):
        if "Prefer model B" in input_text:
            return {"response": "This is a thoughtful answer from Model B"}
        return {"response": "I don't know"}

class PreferenceScorer(Scorer):
    @weave.op
    async def _get_other_model_output(self, example: dict) -> Any:
        """Get output from the other model for comparison.
        Args:
            example: The input example data to run through the other model
        Returns:
            The output from the other model
        """

        other_model_result = await apply_model_async(
            self.other_model,
            example,
            None,
        )

        if isinstance(other_model_result, ApplyModelError):
            return None

        return other_model_result.model_output

    @weave.op
    async def score(self, output: dict, input_text: str) -> dict:
        """Compare the output of the primary model with the other model.
        Args:
            output (dict): The output from the primary model.
            input_text (str): The input text used to generate the outputs.
        Returns:
            dict: A flat dictionary containing the comparison result and reason.
        """
        other_output = await self._get_other_model_output(
            {"input_text": input_text}
        )
        if other_output is None:
            return {"primary_is_better": False, "reason": "Other model failed"}

        if "Prefer model A" in input_text:
            primary_is_better = True
            reason = "Model A gave a great answer"
        else:
            primary_is_better = False
            reason = "Model B is preferred for this type of question"

        return {"primary_is_better": primary_is_better, "reason": reason}

dataset = Dataset(
    rows=[
        {"input_text": "Prefer model A: Question 1"},  # Model A wins
        {"input_text": "Prefer model A: Question 2"},  # Model A wins
        {"input_text": "Prefer model B: Question 3"},  # Model B wins
        {"input_text": "Prefer model B: Question 4"},  # Model B wins
    ]
)

model_a = ModelA()
model_b = ModelB()
pref_scorer = PreferenceScorer(other_model=model_b)
evaluation = Evaluation(dataset=dataset, scorers=[pref_scorer])
evaluation.evaluate(model_a)
```
