---
- title: Evaluations
- description: アプリケーションを体系的に改善するための評価駆動型LLMアプリケーション開発
---

*評価駆動型LLMアプリケーション開発*は、一貫した厳選された例を使用して体系的に動作を測定することで、LLMアプリケーションを体系的に改善するのに役立ちます。

Weaveでは、ワークフローの中核&#x306F;*`Evaluation`オブジェクト*であり、以下を定義します：

* テスト例の[`Dataset`](../core-types/datasets)または辞書のリスト。
* 1つ以上の[スコアリング関数](../evaluation/scorers.mdx)。
* オプションの設定（[入力前処理](#format-dataset-rows-before-evaluating)など）。

一度`Evaluation`を定義したら、[`Model`](../core-types/models.mdx)オブジェクトまたはLLMアプリケーションロジックを含むカスタム関数に対して実行できます。`.evaluate()`の各呼び出しは*評価実行*をトリガーします。`Evaluation`オブジェクトを設計図として、各実行をそのセットアップの下でアプリケーションがどのようにパフォーマンスを発揮するかの測定と考えてください。

評価を開始するには、次の手順を完了してください：

1. [オブジェクトを作成する`Evaluation`オブジェクト](#1-create-an-evaluation-object)
2. [例のデータセットを定義する](#2-define-a-datset-of-test-examples)
3. [スコアリング関数を定義する](#3-define-scoring-functions)
4. [評価する`Model`を定義する](#4-define-a-model-to-evaluate)
5. [評価を実行する](#5-run-the-evaluation)

完全な評価コードサンプルは[こちら](#full-evaluation-code-sample)で見つけることができます。また、[高度な評価機能](#advanced-evaluation-usage)（[保存されたビュー](#saved-views)や[命令型評価](#imperative-evaluations-evaluationlogger)など）についても詳しく学ぶことができます。

## 1. `Evaluation`オブジェクトを作成する

オブジェクトを作成することは、評価設定をセットアップする最初のステップです。`Evaluation`は、例データ、スコアリングロジック、およびオプションの前処理で構成されています。後で、これを使用して1つ以上の評価を実行します。`Evaluation`Weaveは各例を取り、アプリケーションを通じて渡し、複数のカスタムスコアリング関数で出力をスコアリングします。これにより、アプリケーションのパフォーマンスの概要と、個々の出力とスコアを詳しく調査するための豊富なUIが得られます。

Weaveは各例を取り、アプリケーションを通じて渡し、複数のカスタムスコアリング関数で出力をスコアリングします。これにより、アプリケーションのパフォーマンスの概要と、個々の出力とスコアを詳しく調査するための豊富なUIが得られます。

### （オプション）カスタム命名

評価フローにはカスタマイズ可能な名前の2種類があります：

* [*評価オブジェクト名*（`evaluation_name`）](#name-the-evaluation-object)：設定された`Evaluation`オブジェクトの永続的なラベル。
* [*評価実行表示名*（`__weave["display_name"]`)](#name-individual-evaluation-runs): UIに表示される特定の評価実行のラベル。

#### 名前を付ける `Evaluation` オブジェクト

オブジェクト `Evaluation` 自体に名前を付けるには、`evaluation_name` パラメータを `Evaluation` クラスに渡します。この名前は、コードとUIリストで評価を識別するのに役立ちます。

```python
evaluation = Evaluation(
    dataset=examples, scorers=[match_score1], evaluation_name="My Evaluation"
)
```

#### 個々の評価実行に名前を付ける

特定の評価実行（`evaluate()`の呼び出し）に名前を付けるには、`__weave` 辞書に `display_name`を使用します。これはその実行のUIに表示される内容に影響します。

```python
evaluation = Evaluation(
    dataset=examples, scorers=[match_score1]
)
evaluation.evaluate(model, __weave={"display_name": "My Evaluation Run"})
```

## 2. テスト例のデータセットを定義する

まず、[Dataset](../core-types/datasets.mdx) オブジェクトまたは評価される例のコレクションを持つ辞書のリストを定義します。これらの例は、テストしたい失敗ケースであることが多く、テスト駆動開発（TDD）の単体テストに似ています。

以下の例は、辞書のリストとして定義されたデータセットを示しています：

```python
examples = [
    {"question": "What is the capital of France?", "expected": "Paris"},
    {"question": "Who wrote 'To Kill a Mockingbird'?", "expected": "Harper Lee"},
    {"question": "What is the square root of 64?", "expected": "8"},
]
```

## 3. スコアリング関数を定義する

次に、1つ以上の [スコアリング関数](../evaluation/scorers.mdx)を作成します。これらは `Dataset`の各例をスコアリングするために使用されます。各スコアリング関数には `output`が必要で、スコアを含む辞書を返す必要があります。オプションで、例から他の入力を含めることもできます。

スコアリング関数には `output` キーワード引数が必要ですが、他の引数はユーザー定義であり、データセット例から取得されます。引数名に基づいた辞書キーを使用して、必要なキーのみを取得します。

<Tip>
  スコアラーが `output` 引数を期待しているのに受け取っていない場合は、レガシーの `model_output` キーを使用している可能性があるかどうかを確認してください。これを修正するには、スコアラー関数を更新して、outputをキーワード引数として使用するようにします。
</Tip>

以下のサンプルスコアラー関数 `match_score1` は `expected` の値を `examples` 辞書からスコアリングに使用します。

```python
import weave

# Collect your examples
examples = [
    {"question": "What is the capital of France?", "expected": "Paris"},
    {"question": "Who wrote 'To Kill a Mockingbird'?", "expected": "Harper Lee"},
    {"question": "What is the square root of 64?", "expected": "8"},
]

# Define any custom scoring function
@weave.op()
def match_score1(expected: str, output: dict) -> dict:
    # Here is where you'd define the logic to score the model output
    return {'match': expected == output['generated_text']}
```

### （オプション）カスタム `Scorer` クラスを定義する

一部のアプリケーションでは、カスタム `Scorer` クラスを作成したい場合があります - 例えば、標準化された `LLMJudge` クラスを特定のパラメータ（例：チャットモデル、プロンプト）、各行の特定のスコアリング、および集計スコアの特定の計算で作成する必要がある場合などです。

詳細については、`Scorer` クラスの定義に関するチュートリアルを [RAGアプリケーションのモデルベース評価](/ja/tutorial-rag#optional-defining-a-scorer-class) で参照してください。

## 4. 評価する `Model` を定義する

を評価するには、`Model`に対して `evaluate` を呼び出し、`Evaluation`を使用します。`Models` は、実験したいパラメータがあり、weaveでキャプチャしたい場合に使用されます。

```python
from weave import Model, Evaluation
import asyncio

class MyModel(Model):
    prompt: str

    @weave.op()
    def predict(self, question: str):
        # here's where you would add your LLM call and return the output
        return {'generated_text': 'Hello, ' + self.prompt}

model = MyModel(prompt='World')

evaluation = Evaluation(
    dataset=examples, scorers=[match_score1]
)
weave.init('intro-example') # begin tracking results with weave
asyncio.run(evaluation.evaluate(model))
```

これにより、各例に対して `predict` が実行され、各スコアリング関数で出力がスコアリングされます。

### （オプション）評価する関数を定義する

あるいは、`@weave.op()`でトラッキングされたカスタム関数を評価することもできます。

```python
@weave.op
def function_to_evaluate(question: str):
    # here's where you would add your LLM call and return the output
    return  {'generated_text': 'some response'}

asyncio.run(evaluation.evaluate(function_to_evaluate))
```

## 5. 評価を実行する

評価を実行するには、評価したいオブジェクトに対して `.evaluate()` を呼び出します。
例えば、`Evaluation` オブジェクトが `evaluation` という名前で、評価する `Model` オブジェクトが `model` という名前の場合、以下のコードで評価実行をインスタンス化します。

```python
asyncio.run(evaluation.evaluate(model))
```

<Tip>
  **評価実行のヒント**

  1. `evaluate()` メソッドはすべての例にわたる結果の要約を返します。出力とスコアを含むスコアリングされた行の完全なセットにアクセスするには、`get_eval_results()`を使用します。
  2. `display_name` を呼び出す際に `.evaluate()` を提供しない場合、Weaveは日付とランダムな覚えやすい名前を使用して自動的に生成します。詳細については、[個々の評価実行に名前を付ける方法](#name-individual-evaluation-runs)を参照してください。
  3. `.evaluate` に渡されるモデルは `Model` または `@weave.op` でトラッキングされた関数である必要があります。通常のPython関数は `@weave.op` でラップされていない限りサポートされていません。
</Tip>

### （オプション）複数の試行を実行する

`trials` パラメータを `Evaluation` オブジェクトに設定して、各例を複数回実行することができます。

```python
evaluation = Evaluation(dataset=examples, scorers=[match_score], trials=3)
```

各例は `model` に3回渡され、各実行は個別にスコアリングされ、Weaveに表示されます。

## 完全な評価コードサンプル

以下のコードサンプルは、最初から最後までの完全な評価実行を示しています。`examples` 辞書は `match_score1` および `match_score2` スコアリング関数によって使用され、`MyModel` の値に基づいて `prompt` を評価し、カスタム関数 `function_to_evaluate` も評価します。`Model` と関数の両方の評価実行は `asyncio.run(evaluation.evaluate()` を介して呼び出されます。

```python
from weave import Evaluation, Model
import weave
import asyncio
weave.init('intro-example')
examples = [
    {"question": "What is the capital of France?", "expected": "Paris"},
    {"question": "Who wrote 'To Kill a Mockingbird'?", "expected": "Harper Lee"},
    {"question": "What is the square root of 64?", "expected": "8"},
]

@weave.op()
def match_score1(expected: str, output: dict) -> dict:
    return {'match': expected == output['generated_text']}

@weave.op()
def match_score2(expected: dict, output: dict) -> dict:
    return {'match': expected == output['generated_text']}

class MyModel(Model):
    prompt: str

    @weave.op()
    def predict(self, question: str):
        # here's where you would add your LLM call and return the output
        return {'generated_text': 'Hello, ' + question + self.prompt}

model = MyModel(prompt='World')
evaluation = Evaluation(dataset=examples, scorers=[match_score1, match_score2])

asyncio.run(evaluation.evaluate(model))

@weave.op()
def function_to_evaluate(question: str):
    # here's where you would add your LLM call and return the output
    return  {'generated_text': 'some response' + question}

asyncio.run(evaluation.evaluate(function_to_evaluate("What is the capitol of France?")))
```

![Evals hero](../../images/evals-hero.png)

## 高度な評価の使用法

### 評価前にデータセット行をフォーマットする

<Warning>
  `preprocess_model_input` 関数は、モデルの予測関数に渡す前に入力にのみ適用されます。スコアラー関数は常に、前処理が適用されていない元のデータセット例を受け取ります。
</Warning>

`preprocess_model_input` パラメータを使用すると、評価関数に渡される前にデータセット例を変換できます。これは以下の場合に役立ちます：

* モデルの予想される入力と一致するようにフィールドの名前を変更する
* データを正しい形式に変換する
* フィールドを追加または削除する
* 各例の追加データを読み込む

以下は `preprocess_model_input` を使用してフィールドの名前を変更する方法を示す簡単な例です：

```python
import weave
from weave import Evaluation
import asyncio

# Our dataset has "input_text" but our model expects "question"
examples = [
    {"input_text": "What is the capital of France?", "expected": "Paris"},
    {"input_text": "Who wrote 'To Kill a Mockingbird'?", "expected": "Harper Lee"},
    {"input_text": "What is the square root of 64?", "expected": "8"},
]

@weave.op()
def preprocess_example(example):
    # Rename input_text to question
    return {
        "question": example["input_text"]
    }

@weave.op()
def match_score(expected: str, output: dict) -> dict:
    return {'match': expected == output['generated_text']}

@weave.op()
def function_to_evaluate(question: str):
    return {'generated_text': f'Answer to: {question}'}

# Create evaluation with preprocessing
evaluation = Evaluation(
    dataset=examples,
    scorers=[match_score],
    preprocess_model_input=preprocess_example
)

# Run the evaluation
weave.init('preprocessing-example')
asyncio.run(evaluation.evaluate(function_to_evaluate))
```

この例では、データセットには `input_text` フィールドを持つ例が含まれていますが、評価関数は `question` パラメータを期待しています。`preprocess_example` 関数はフィールドの名前を変更することで各例を変換し、評価が正しく機能するようにします。

前処理関数は：

1. データセットから生の例を受け取ります
2. モデルが期待するフィールドを持つ辞書を返します
3. 評価関数に渡される前に各例に適用されます

これは、モデルが期待するものとは異なるフィールド名や構造を持つ可能性がある外部データセットを扱う場合に特に役立ちます。

### 評価でHuggingFaceデータセットを使用する

私たちはサードパーティのサービスやライブラリとの統合を継続的に改善しています。

より円滑な統合を構築している間、`preprocess_model_input` を一時的な回避策としてWeave評価でHuggingFace Datasetsを使用することができます。

現在のアプローチについては、[評価でHuggingFaceデータセットを使用するクックブック](/ja/cookbooks/hf_dataset_evals) を参照してください。

### 保存されたビュー

Evalsテーブルの設定、フィルター、並べ替えを*保存されたビュー*として保存し、お好みの設定にすぐにアクセスできます。UIとPython SDKを通じて保存されたビューを設定およびアクセスできます。詳細については、[Saved Views](/ja/guides/tools/saved-views)をご覧ください。

### 命令型評価（`EvaluationLogger`）

より柔軟な評価フレームワークを希望する場合は、Weaveの[`EvaluationLogger`](../evaluation/evaluation_logger.mdx)をご確認ください。命令型アプローチは複雑なワークフローに対してより柔軟性を提供し、標準評価フレームワークはより構造化されたガイダンスを提供します。
