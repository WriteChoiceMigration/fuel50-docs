---
- title: 실험 생성하기
- description: W&B Experiment를 생성합니다.
---

W\&B Python SDK를 사용하여 머신 러닝 실험을 추적하세요. 그런 다음 대화형 대시보드에서 결과를 검토하거나 [W\&B Public API](/ref/python/public-api/)를 사용하여 프로그래밍 방식으로 액세스하기 위해 데이터를 Python으로 내보낼 수 있습니다.

이 가이드는 W\&B 구성 요소를 사용하여 W\&B 실험을 생성하는 방법을 설명합니다.

## W\&B 실험을 생성하는 방법

네 단계로 W\&B 실험을 생성하세요:

<CardGroup>
  <Card title="W&B Run 초기화하기" icon="play" href="#initialize-a-wb-run" horizontal iconType="solid" />

  <Card title="하이퍼파라미터 딕셔너리 캡처하기" icon="sliders" href="#capture-a-dictionary-of-hyperparameters" horizontal iconType="solid" />

  <Card title="훈련 루프 내에서 메트릭 로깅하기" icon="chart-line" href="#log-metrics-inside-your-training-loop" horizontal iconType="solid" />

  <Card title="W&B에 아티팩트 로깅하기" icon="chart-line" href="#log-an-artifact-to-wb" horizontal iconType="solid" />
</CardGroup>

### W\&B run 초기화하기

W\&B Run을 생성하려면 [`wandb.init()`](/ref/python/init)를 사용하세요.

다음 코드 스니펫은 `“cat-classification”`라는 이름의 새 W\&B 프로젝트를 생성하고 `“My first experiment”`라는 설명을 추가하여 이 실행을 식별하는 데 도움을 줍니다. `“baseline”`와 `“paper1”`태그가 포함되어 이 실행이 향후 논문 출판을 위한 기준 실험임을 상기시킵니다.

```python
import wandb

with wandb.init(
    project="cat-classification",
    notes="My first experiment",
    tags=["baseline", "paper1"],
) as run:
    ...
```

`wandb.init()`는 [Run](https://docs.wandb.ai/ref/python/run/) 객체를 반환합니다

<Note>
  Note: Runs are added to pre-existing projects if that project already exists when you call wandb.init(). For example, if you already have a project called `“cat-classification”`, 해당 프로젝트는 계속 존재하며 삭제되지 않습니다. 대신, 새로운 실행이 해당 프로젝트에 추가됩니다.
</Note>

### 하이퍼파라미터 딕셔너리 캡처하기

학습률이나 모델 유형과 같은 하이퍼파라미터 딕셔너리를 저장하세요. config에 캡처한 모델 설정은 나중에 결과를 구성하고 쿼리하는 데 유용합니다.

```python
with wandb.init(
    ...,
    config={"epochs": 100, "learning_rate": 0.001, "batch_size": 128},
) as run:
    ...
```

실험 구성 방법에 대한 자세한 내용은 [Configure Experiments](/guides/track/config)를 참조하세요.

### 훈련 루프 내에서 메트릭 로깅하기

정확도와 손실과 같은 각 훈련 단계에 대한 메트릭을 로깅하려면 `run.log()`를 호출하세요.

```python
model, dataloader = get_model(), get_data()

for epoch in range(run.config.epochs):
    for batch in dataloader:
        loss, accuracy = model.training_step()
        run.log({"accuracy": accuracy, "loss": loss})
```

W\&B로 로깅할 수 있는 다양한 데이터 유형에 대한 자세한 내용은 [Log Data During Experiments](/guides/track/log)를 참조하세요.

### W\&B에 아티팩트 로깅하기

선택적으로 W\&B Artifact를 로깅하세요. Artifacts를 사용하면 데이터셋과 모델을 쉽게 버전 관리할 수 있습니다.

```python
# You can save any file or even a directory. In this example, we pretend
# the model has a save() method that outputs an ONNX file.
model.save("path_to_model.onnx")
run.log_artifact("path_to_model.onnx", name="trained-model", type="model")
```

자세한 내용은 [Artifacts](/guides/artifacts)나 [Registry](/guides/model_registry)에서 모델 버전 관리에 대해 알아보세요.

### 모두 종합하기

앞서 언급한 코드 스니펫이 포함된 전체 스크립트는 아래에서 확인할 수 있습니다:

```python
import wandb

with wandb.init(
    project="cat-classification",
    notes="",
    tags=["baseline", "paper1"],
    # Record the run's hyperparameters.
    config={"epochs": 100, "learning_rate": 0.001, "batch_size": 128},
) as run:
    # Set up model and data.
    model, dataloader = get_model(), get_data()

    # Run your training while logging metrics to visualize model performance.
    for epoch in range(run.config["epochs"]):
        for batch in dataloader:
            loss, accuracy = model.training_step()
            run.log({"accuracy": accuracy, "loss": loss})

    # Upload the trained model as an artifact.
    model.save("path_to_model.onnx")
    run.log_artifact("path_to_model.onnx", name="trained-model", type="model")
```

## 다음 단계: 실험 시각화하기

W\&B 대시보드를 머신 러닝 모델의 결과를 구성하고 시각화하는 중앙 장소로 사용하세요. 몇 번의 클릭만으로 [parallel coordinates plots](/guides/app/features/panels/parallel-coordinates),[ parameter importance analyzes](/guides/app/features/panels/parameter-importance), 그리고 [more](/guides/app/features/panels)와 같은 풍부하고 대화형 차트를 구성할 수 있습니다.

<Frame>
  <img src="/images/guides/track/assets/images/quickstart_dashboard_example-fb99024c6523fdc99a8e03c16398ca28.png" />
</Frame>

실험 및 특정 실행을 보는 방법에 대한 자세한 내용은 [Visualize results from experiments](/guides/track/workspaces)를 참조하세요.

## 모범 사례

실험을 생성할 때 고려해야 할 몇 가지 제안 지침은 다음과 같습니다:

1. **실행 완료하기**: 사용자 `wandb.init()` in a `with` 코드가 완료되거나 예외가 발생할 때 자동으로 실행을 종료로 표시하는 명령문입니다.
   * Jupyter 노트북에서는 Run 객체를 직접 관리하는 것이 더 편리할 수 있습니다. 이 경우 다음과 같이 명시적으로 `finish()` Run 객체에서 호출하여 완료로 표시할 수 있습니다:
     ```python
     # In a notebook cell:
     run = wandb.init()

     # In a different cell:
     run.finish()
     ```
2. **Config**: 하이퍼파라미터, 아키텍처, 데이터셋 및 모델을 재현하는 데 사용하고 싶은 다른 요소들을 추적합니다. 이들은 열로 표시됩니다 - config 열을 사용하여 앱에서 실행을 동적으로 그룹화하고, 정렬하고, 필터링할 수 있습니다.
3. **Project**: 프로젝트는 함께 비교할 수 있는 실험 세트입니다. 각 프로젝트는 전용 대시보드 페이지를 가지며, 다른 모델 버전을 비교하기 위해 다양한 실행 그룹을 쉽게 켜고 끌 수 있습니다.
4. **Notes**: 자신을 위한 간단한 커밋 메시지입니다. 노트는 스크립트에서 설정할 수 있습니다. W\&B 앱의 프로젝트 대시보드 개요 섹션에서 나중에 노트를 편집할 수 있습니다.
5. **Tags**: 기준 실행과 즐겨찾기 실행을 식별합니다. 태그를 사용하여 실행을 필터링할 수 있습니다. W\&B 앱의 프로젝트 대시보드 개요 섹션에서 나중에 태그를 편집할 수 있습니다.
6. **실험을 비교하기 위한 여러 실행 세트 생성**: 실험을 비교할 때 여러 실행 세트를 만들어 메트릭을 쉽게 비교할 수 있습니다. 동일한 차트나 차트 그룹에서 실행 세트를 켜거나 끌 수 있습니다.

다음 코드 스니펫은 위에 나열된 모범 사례를 사용하여 W\&B Experiment를 정의하는 방법을 보여줍니다:

```python

import wandb

config = {
    "learning_rate": 0.01,
    "momentum": 0.2,
    "architecture": "CNN",
    "dataset_id": "cats-0192",
}

with wandb.init(
    project="detect-cats",
    notes="tweak baseline",
    tags=["baseline", "paper1"],
    config=config,
) as run:
    ...
```

W\&B Experiment를 정의할 때 사용 가능한 매개변수에 대한 자세한 정보는 [`wandb.init`](/ref/python/init) API 문서를 [API Reference Guide](/ref/python/)에서 참조하세요.
