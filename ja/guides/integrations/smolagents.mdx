# Smolagents

<Warning>
  このページに示されているすべてのコードサンプルはPythonで書かれています。
</Warning>

このページでは、[Smolagents](https://huggingface.co/docs/smolagents/en/index)をW\&B Weaveと統合して、エージェントアプリケーションを追跡および分析する方法について説明します。モデル推論のログ記録、関数呼び出しのモニタリング、Weaveのトレースおよびバージョン管理機能を使用した実験の整理方法を学びます。提供される例に従うことで、貴重な洞察をキャプチャし、アプリケーションを効率的にデバッグし、異なるモデル構成を比較することができます—すべてWeaveウェブインターフェース内で行えます。

## 概要

Smolagentsは、強力なエージェントアプリケーションを構築するための最小限の抽象化を提供するシンプルなフレームワークです。OpenAI、Hugging Face Transformers、Anthropicなど、複数のLLMプロバイダーをサポートしています。

Weaveは自動的にトレースをキャプチャします [Smolagents](https://huggingface.co/docs/smolagents/en/index)。追跡を開始するには、`weave.init()`を呼び出し、通常通りライブラリを使用します。

## 前提条件

1. SmolagentsをWeaveで使用する前に、必要なライブラリをインストールするか、最新バージョンにアップグレードしてください。次のコマンドは`smolagents`、`openai`、および`weave`をインストールまたはアップグレードし、出力を抑制します：

   ```python
   pip install -U smolagents openai weave -qqq
   ```

2. SmolagentsはOpenAI、Hugging Face Transformers、Anthropicなど、複数のLLMプロバイダーをサポートしています。選択したプロバイダーのAPIキーを対応する環境変数を設定して設定します：

   ```python
   import os
   import getpass

   os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
   ```

## 基本的なトレース

言語モデルアプリケーションのトレースを中央の場所に保存することは、開発中および本番環境で不可欠です。これらのトレースはデバッグに役立ち、アプリケーションを改善するための貴重なデータセットとして機能します。

Weaveは自動的にトレースをキャプチャします [Smolagents](https://huggingface.co/docs/smolagents/en/index)。追跡を開始するには、`weave.init()`を呼び出してWeaveを初期化し、通常通りライブラリを使用します。

次の例は、Weaveでツールを使用するLLMエージェントへの推論呼び出しをログに記録する方法を示しています。このシナリオでは：

* Smolagentsの`gpt-4o`を使用して言語モデル（OpenAIの`OpenAIServerModel`）を定義します。
* エージェントが必要に応じて呼び出せる検索ツール（`DuckDuckGoSearchTool`）を設定します。
* ツールとモデルを渡して`ToolCallingAgent`を構築します。
* 検索ツールをトリガーするクエリをエージェントを通じて実行します。
* Weaveは各関数とモデルの呼び出しをログに記録し、ウェブインターフェースを通じて検査できるようにします。

```python
import weave
from smolagents import DuckDuckGoSearchTool, OpenAIServerModel, ToolCallingAgent

# Initialize Weave
weave.init(project_name="smolagents")

# Define your LLM provider supported by Smolagents
model = OpenAIServerModel(model_id="gpt-4o")

# Define a DuckDuckGo web search tool based on your query
search_tool = DuckDuckGoSearchTool()

# Define a tool-calling agent
agent = ToolCallingAgent(tools=[search_tool], model=model)
answer = agent.run(
    "Get me just the title of the page at url 'https://wandb.ai/geekyrakshit/story-illustration/reports/Building-a-GenAI-assisted-automatic-story-illustrator--Vmlldzo5MTYxNTkw'?"
)
```

コードサンプルを実行したら、Weaveプロジェクトダッシュボードに移動してトレースを表示します。

![Weave logs each inference call, providing details about inputs, outputs, and metadata.](./imgs/smolagents-trace.png)

## カスタムツールのトレース

エージェントワークフロー用のカスタムツールを宣言するには、`@tool`で関数をデコレートするか、`smolagents`から`smolagents.Tool`クラスを継承します。

Weaveは自動的にSmolagentsワークフロー用のカスタムツール呼び出しを追跡します。次の例は、WeaveでカスタムSmolagentsツール呼び出しをログに記録する方法を示しています：

* カスタム`get_weather`関数が定義され、Smolagentsから`@tool`でデコレートされ、エージェントが推論プロセスの一部としてそれを呼び出せるようにします。
* この関数は場所と摂氏出力用のオプションフラグを受け入れます。
* 言語モデルは`OpenAIServerModel`を使用してインスタンス化されます。
* カスタムツールとモデルで`ToolCallingAgent`が作成されます。
* エージェントがクエリを実行すると、`get_weather`ツールを選択して呼び出します。
* Weaveはモデル推論とカスタムツールの呼び出しの両方を、引数と戻り値を含めてログに記録します。

```python
from typing import Optional

import weave
from smolagents import OpenAIServerModel, ToolCallingAgent, tool

weave.init(project_name="smolagents")

@tool
def get_weather(location: str, celsius: Optional[bool] = False) -> str:
    """
    Get the weather in the next few days for a given location.
    Args:
        location: The location.
        celsius: Whether to use Celsius for temperature.
    """
    return f"The weather in {location} is sunny with temperatures around 7°C."

model = OpenAIServerModel(model_id="gpt-4o")
agent = ToolCallingAgent(tools=[get_weather], model=model)
answer = agent.run("What is the weather in Tokyo?")
```

コードサンプルを実行したら、Weaveプロジェクトダッシュボードに移動してトレースを表示します。

![Weave logs each custom tool call.](./imgs/smolagents-custom-tool.png)
