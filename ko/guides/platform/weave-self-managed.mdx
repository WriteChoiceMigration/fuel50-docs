---
- title: W&B Weave Self-Managed
- description: 자체 인프라에서 Weave 배포 및 관리
---

<Warning>
  Weave on self-managed infrastructure is currently in Private Preview.

  프로덕션 환경의 경우, W\&B는 강력하게 다음을 권장합니다 [W\&B Dedicated Cloud](https://docs.wandb.ai/guides/hosting/hosting-options/dedicated_cloud), Weave가 정식 출시되어 있는 곳입니다.

  프로덕션 수준의 자체 관리형 인스턴스를 배포하려면 다음으로 문의하세요 `support@wandb.com`.
</Warning>

이 가이드는 자체 관리형 환경에서 W\&B Weave를 실행하는 데 필요한 모든 구성 요소를 배포하는 방법을 설명합니다.

자체 관리형 Weave 배포의 핵심 구성 요소는 [ClickHouseDB](https://clickhouse.com/)로, Weave 애플리케이션 백엔드가 이에 의존합니다.

배포 프로세스가 완전히 기능하는 ClickHouseDB 인스턴스를 설정하지만, 프로덕션 환경에서의 신뢰성과 고가용성을 보장하기 위해 추가 단계가 필요할 수 있습니다.

## 요구 사항

* W\&B Platform이 설치되어 있어야 합니다. 자세한 내용은 [Self-Managed Deployment Guide](https://docs.wandb.ai/guides/hosting/hosting-options/self-managed/)를 참조하세요.
* [Bitnami's ClickHouse Helm Chart](https://github.com/bitnami/charts/tree/main/bitnami/clickhouse).
* ClickHouse 스토리지용으로 미리 구성된 S3 버킷. 구성 세부 정보는 [Provide S3 Credentials](#provide-s3-credentials)를 참조하세요.
* 다음 사양을 갖춘 Kubernetes 클러스터 노드:
  * CPU: 8 cores
  * RAM: 64 GB
  * Disk: 200GB+
* W\&B에서 제공하는 Weave 지원 라이선스. 라이선스를 요청하려면 다음으로 문의하세요 `support@wandb.com`.

<Tip>
  자세한 참조 아키텍처는 [https://docs.wandb.ai/guides/hosting/self-managed/ref-arch/](https://docs.wandb.ai/guides/hosting/self-managed/ref-arch/#models-and-weave)를 참조하세요.
</Tip>

## 1. ClickHouse 구성

이 문서의 ClickHouse 배포는 [Bitnami ClickHouse](https://bitnami.com/stack/clickhouse) 패키지를 사용합니다.

Bitnami Helm 차트는 기본 ClickHouse 기능, 특히 [ClickHouse Keeper](https://clickhouse.com/docs/en/guides/sre/keeper/clickhouse-keeper)의 사용에 대한 좋은 지원을 제공합니다.

Clickhouse를 구성하려면 다음 단계를 완료하세요:

1. [Helm 저장소 구성](#configure-helm-repository)
2. [Helm 구성 생성](#create-helm-configuration)
3. [S3 자격 증명 제공](#provide-s3-credentials)

### Helm 저장소 구성

1. Bitnami Helm 저장소 추가:

   `helm repo add bitnami https://charts.bitnami.com/bitnami`

2. 저장소 업데이트:

   `helm repo update`

### Helm 구성 생성

Helm 구성의 가장 중요한 부분은 XML 형식으로 제공되는 ClickHouse 구성입니다. 아래는 필요에 맞게 사용자 정의할 수 있는 매개변수가 있는 예제 `values.yaml` 파일입니다.
구성 프로세스를 쉽게 하기 위해 관련 섹션에 `{/* COMMENT */}` 형식으로 주석을 추가했습니다.

다음 매개변수를 수정하세요:

* `clusterName`
* `auth.username`
* `auth.password`
* S3 버킷 관련 구성

W\&B는 `clusterName` 값을 `values.yaml`에서 `weave_cluster`로 유지하는 것을 권장합니다. 이는 W\&B Weave가 데이터베이스 마이그레이션을 실행할 때 예상되는 클러스터 이름입니다. 다른 이름을 사용해야 하는 경우 [Setting `clusterName`](#setting-clustername) 섹션에서 자세한 정보를 참조하세요.

```yaml
## @param clusterName ClickHouse cluster name
clusterName: weave_cluster

## @param shards Number of ClickHouse shards to deploy
shards: 1

## @param replicaCount Number of ClickHouse replicas per shard to deploy
## if keeper enable, same as keeper count, keeper cluster by shards.
replicaCount: 3

persistence:
  enabled: true
  size: 30G # this size must be larger than cache size.

## ClickHouse resource requests and limits
resources:
  requests:
    cpu: 0.5
    memory: 500Mi
  limits:
    cpu: 3.0
    memory: 6Gi

## Authentication
auth:
  username: weave_admin
  password: "weave_123"
  existingSecret: ""
  existingSecretKey: ""

## @param logLevel Logging level
logLevel: information

## @section ClickHouse keeper configuration parameters
keeper:
  enabled: true

## @param extraEnvVars Array with extra environment variables to add to ClickHouse nodes
##
extraEnvVars:
  - name: S3_ENDPOINT
    value: "https://s3.us-east-1.amazonaws.com/bucketname/$(CLICKHOUSE_REPLICA_ID)"


## @param defaultConfigurationOverrides [string] Default configuration overrides (evaluated as a template)
defaultConfigurationOverrides: |
  <clickhouse>
    {/* Macros */}
    <macros>
      <shard from_env="CLICKHOUSE_SHARD_ID"></shard>
      <replica from_env="CLICKHOUSE_REPLICA_ID"></replica>
    </macros>
    {/* Log Level */}
    <logger>
      <level>{{ .Values.logLevel }}</level>
    </logger>
    {{- if or (ne (int .Values.shards) 1) (ne (int .Values.replicaCount) 1)}}
    <remote_servers>
      <{{ .Values.clusterName }}>
        {{- $shards := $.Values.shards | int }}
        {{- range $shard, $e := until $shards }}
        <shard>
          <internal_replication>true</internal_replication>
          {{- $replicas := $.Values.replicaCount | int }}
          {{- range $i, $_e := until $replicas }}
          <replica>
            <host>{{ printf "%s-shard%d-%d.%s.%s.svc.%s" (include "common.names.fullname" $ ) $shard $i (include "clickhouse.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
            <port>{{ $.Values.service.ports.tcp }}</port>
          </replica>
          {{- end }}
        </shard>
        {{- end }}
      </{{ .Values.clusterName }}>
    </remote_servers>
    {{- end }}
    {{- if .Values.keeper.enabled }}
    <keeper_server>
      <tcp_port>{{ $.Values.containerPorts.keeper }}</tcp_port>
      {{- if .Values.tls.enabled }}
      <tcp_port_secure>{{ $.Values.containerPorts.keeperSecure }}</tcp_port_secure>
      {{- end }}
      <server_id from_env="KEEPER_SERVER_ID"></server_id>
      <log_storage_path>/bitnami/clickhouse/keeper/coordination/log</log_storage_path>
      <snapshot_storage_path>/bitnami/clickhouse/keeper/coordination/snapshots</snapshot_storage_path>
      <coordination_settings>
        <operation_timeout_ms>10000</operation_timeout_ms>
        <session_timeout_ms>30000</session_timeout_ms>
        <raft_logs_level>trace</raft_logs_level>
      </coordination_settings>
      <raft_configuration>
        {{- $nodes := .Values.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <server>
          <id>{{ $node | int }}</id>
          <hostname from_env="{{ printf "KEEPER_NODE_%d" $node }}"></hostname>
          <port>{{ $.Values.service.ports.keeperInter }}</port>
        </server>
        {{- end }}
      </raft_configuration>
    </keeper_server>
    {{- end }}
    {{- if or .Values.keeper.enabled .Values.zookeeper.enabled .Values.externalZookeeper.servers }}
    <zookeeper>
      {{- if or .Values.keeper.enabled }}
      {{- $nodes := .Values.replicaCount | int }}
      {{- range $node, $e := until $nodes }}
      <node>
        <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
        <port>{{ $.Values.service.ports.keeper }}</port>
      </node>
      {{- end }}
      {{- else if .Values.zookeeper.enabled }}
      {{- $nodes := .Values.zookeeper.replicaCount | int }}
      {{- range $node, $e := until $nodes }}
      <node>
        <host from_env="{{ printf "KEEPER_NODE_%d" $node }}"></host>
        <port>{{ $.Values.zookeeper.service.ports.client }}</port>
      </node>
      {{- end }}
      {{- else if .Values.externalZookeeper.servers }}
      {{- range $node :=.Values.externalZookeeper.servers }}
      <node>
        <host>{{ $node }}</host>
        <port>{{ $.Values.externalZookeeper.port }}</port>
      </node>
      {{- end }}
      {{- end }}
    </zookeeper>
    {{- end }}
    {{- if .Values.metrics.enabled }}
    <prometheus>
      <endpoint>/metrics</endpoint>
      <port from_env="CLICKHOUSE_METRICS_PORT"></port>
      <metrics>true</metrics>
      <events>true</events>
      <asynchronous_metrics>true</asynchronous_metrics>
    </prometheus>
    {{- end }}
    <listen_host>0.0.0.0</listen_host>
    <listen_host>::</listen_host>
    <listen_try>1</listen_try>
    <storage_configuration>
      <disks>
        <s3_disk>
          <type>s3</type>
          <endpoint from_env="S3_ENDPOINT"></endpoint>

          {/* AVOID USE CREDENTIALS CHECK THE RECOMMENDATION */}
          <access_key_id>xxx</access_key_id>
          <secret_access_key>xxx</secret_access_key>
          {/* AVOID USE CREDENTIALS CHECK THE RECOMMENDATION */}

         <metadata_path>/var/lib/clickhouse/disks/s3_disk/</metadata_path>
        </s3_disk>
        <s3_disk_cache>
  	      <type>cache</type>
          <disk>s3_disk</disk>
          <path>/var/lib/clickhouse/s3_disk_cache/cache/</path>
          {/* THE CACHE SIZE MUST BE LOWER THAN PERSISTENT VOLUME */}
          <max_size>20Gi</max_size>
        </s3_disk_cache>
      </disks>
      <policies>
        <s3_main>
          <volumes>
            <main>
              <disk>s3_disk_cache</disk>
            </main>
          </volumes>
        </s3_main>
      </policies>
    </storage_configuration>
    <merge_tree>
      <storage_policy>s3_main</storage_policy>
    </merge_tree>
  </clickhouse>

## @section Zookeeper subchart parameters
zookeeper:
  enabled: false
```

### S3 엔드포인트 구성

각 ClickHouse 복제본이 버킷의 자체 폴더에서 데이터를 읽고 쓸 수 있도록 버킷 엔드포인트를 환경 변수로 설정해야 합니다.

```
extraEnvVars:
  - name: S3_ENDPOINT
    value: "https://s3.us-east-1.amazonaws.com/bucketname/$(CLICKHOUSE_REPLICA_ID)"
```

<Warning>
  버킷 엔드포인트 구성에서 `$(CLICKHOUSE_REPLICA_ID)`를 제거하지 마세요. 이렇게 하면 각 ClickHouse 복제본이 버킷의 자체 폴더에서 데이터를 쓰고 읽을 수 있습니다.
</Warning>

### S3 자격 증명 제공

구성을 하드코딩하거나 ClickHouse가 환경 변수 또는 EC2 인스턴스에서 데이터를 가져오도록 하여 S3 버킷에 액세스하기 위한 자격 증명을 지정할 수 있습니다.

#### 구성 하드코딩

스토리지 구성에 자격 증명을 직접 포함:

```plaintext
<type>s3</type>
<endpoint from_env="S3_ENDPOINT"></endpoint>
<access_key_id>xxx</access_key_id>
<secret_access_key>xxx</secret_access_key>
```

#### 환경 변수 또는 EC2 메타데이터 사용

자격 증명을 하드코딩하는 대신, ClickHouse가 환경 변수 또는 Amazon EC2 인스턴스 메타데이터에서 동적으로 가져오도록 할 수 있습니다.

```plaintext
<use_environment_credentials>true</use_environment_credentials>
```

이에 대한 자세한 내용은 [ClickHouse: Separation of Storage and Compute](https://clickhouse.com/docs/en/guides/separation-storage-compute)에서 확인할 수 있습니다.

## 2. ClickHouse 설치 및 배포

저장소를 설정하고 `values.yaml` 파일을 준비한 후, 다음 단계는 ClickHouse를 설치하는 것입니다.

```bash
helm install clickhouse bitnami/clickhouse -f values.yaml --version 8.0.10
```

<Warning>
  버전 `8.0.10`을 사용하고 있는지 확인하세요. 최신 차트 버전(`9.0.0`)은 이 문서에서 제안하는 구성과 함께 작동하지 않습니다.
</Warning>

## 3. ClickHouse 배포 확인

다음 명령을 사용하여 ClickHouse가 배포되었는지 확인하세요:

```bash
kubectl get pods
```

다음 포드가 표시되어야 합니다:

```bash
NAME                                 READY   STATUS    RESTARTS   AGE
clickhouse-shard0-0                  1/1     Running   0          9m59s
clickhouse-shard0-1                  1/1     Running   0          10m
clickhouse-shard0-2                  1/1     Running   0          10m
```

## 4. Weave 배포

Weave는 이미 [W\&B Operator](https://docs.wandb.ai/guides/hosting/operator/#wb-kubernetes-operator)를 통해 자동 배포가 가능합니다. W\&B Platform이 설치된 상태에서 다음 단계를 완료하세요:

1. 플랫폼 배포에 사용된 [CR instance](https://docs.wandb.ai/guides/hosting/operator/#complete-example)를 편집합니다.
2. Weave 구성을 추가합니다.

## 5. 정보 수집

1. Weave 추적을 구성하기 위해 Kubernetes 서비스 세부 정보를 사용하세요:

* **Endpoint**: `<release-name>-headless.<namespace>.svc.cluster.local`
  * 다음을 대체하세요 `<release-name>` Helm 릴리스 이름으로
  * 다음을 대체하세요 `<namespace>` 당신의 `NAMESPACE`
  * 서비스 세부 정보 가져오기: `kubectl get svc -n <namespace>`
* **Username**: 다음에서 설정 `values.yaml`
* **Password**: 다음에서 설정 `values.yaml`

2. 이 정보를 사용하여 다음 구성을 추가하여 W\&B Platform Custom Resource(CR)를 업데이트하세요:

   ```yaml
   apiVersion: apps.wandb.com/v1
   kind: WeightsAndBiases
   metadata:
     labels:
       app.kubernetes.io/name: weightsandbiases
       app.kubernetes.io/instance: wandb
     name: wandb
     namespace: default
   spec:
     values:
       global:
       [...]
         clickhouse:
           host: <release-name>-headless.<namespace>.svc.cluster.local
           port: 8123
           password: <password>
           user: <username>
           database: wandb_weave
           # `replicated` must be set to `true` if replicating data across multiple nodes
           # This is in preview, use the env var `WF_CLICKHOUSE_REPLICATED`
           replicated: true

         weave-trace:
           enabled: true
       [...]
       weave-trace:
         install: true
         extraEnv:
           WF_CLICKHOUSE_REPLICATED: "true"
       [...]
   ```

<Warning>
  둘 이상의 복제본을 사용할 때(W\&B는 최소 3개의 복제본을 권장함), Weave Traces에 대해 다음 환경 변수가 설정되어 있는지 확인하세요.

  ```
  extraEnv:
    WF_CLICKHOUSE_REPLICATED: "true"
  ```

  이는 `replicated: true`와 동일한 효과가 있으며 미리보기 중입니다.
</Warning>

3. 다음을 설정하세요 `clusterName` 다음에서 `values.yaml`를 `weave_cluster`로 설정하세요. 그렇지 않으면 데이터베이스 마이그레이션이 실패합니다.

   또는 다른 클러스터 이름을 사용하는 경우, 아래 예시와 같이 `WF_CLICKHOUSE_REPLICATED_CLUSTER` 환경 변수를 `weave-trace.extraEnv`에서 선택한 이름과 일치하도록 설정하세요.

   ```yaml
   [...]
     clickhouse:
       host: <release-name>-headless.<namespace>.svc.cluster.local
       port: 8123
       password: <password>
       user: <username>
       database: wandb_weave
       # `replicated` must be set to `true` if replicating data across multiple nodes
       # This is in preview, use the env var `WF_CLICKHOUSE_REPLICATED`
       replicated: true

     weave-trace:
       enabled: true
   [...]
   weave-trace:
     install: true
     extraEnv:
       WF_CLICKHOUSE_REPLICATED: "true"
       WF_CLICKHOUSE_REPLICATED_CLUSTER: "different_cluster_name"
   [...]
   ```

   최종 구성은 다음 예시와 같이 보일 것입니다:

   ```yaml
   apiVersion: apps.wandb.com/v1
   kind: WeightsAndBiases
   metadata:
     labels:
       app.kubernetes.io/name: weightsandbiases
       app.kubernetes.io/instance: wandb
     name: wandb
     namespace: default
   spec:
     values:
       global:
         license: eyJhbGnUzaHgyQjQyQWhEU3...ZieKQ2x5GGfw
         host: https://wandb.example.com

         bucket:
           name: abc-wandb-moving-pipefish
           provider: gcs

         mysql:
           database: wandb_local
           host: 10.218.0.2
           name: wandb_local
           password: 8wtX6cJHizAZvYScjDzZcUarK4zZGjpV
           port: 3306
           user: wandb

         clickhouse:
           host: <release-name>-headless.<namespace>.svc.cluster.local
           port: 8123
           password: <password>
           user: <username>
           database: wandb_weave
           # This option must be true if replicating data across multiple nodes
           replicated: true

         weave-trace:
           enabled: true

       ingress:
         annotations:
           ingress.gcp.kubernetes.io/pre-shared-cert: abc-wandb-cert-creative-puma
           kubernetes.io/ingress.class: gce
           kubernetes.io/ingress.global-static-ip-name: abc-wandb-operator-address

       weave-trace:
         install: true
         extraEnv:
           WF_CLICKHOUSE_REPLICATED: "true"
   ```

4. Custom Resource (CR)가 준비되면 새 구성을 적용하세요:

   ```bash
   kubectl apply -f wandb.yaml
   ```

## 6. Weave 액세스

배포가 실행되면, `host` 옵션에 구성된 W\&B 엔드포인트에 액세스하면 Weave 라이선스 상태가 활성화된 것으로 표시됩니다.

![Weave](../../media/weave-self-managed/weave-org-dashboard.png)
