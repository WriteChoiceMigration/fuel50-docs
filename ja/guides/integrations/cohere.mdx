---
- title: Cohere
- description: Weaveを使用してCohere Pythonライブラリを介して行われたLLM呼び出しを自動的に追跡およびログに記録する
---

<a target="_blank" href="https://colab.research.google.com/github/wandb/examples/blob/master/weave/docs/quickstart_cohere.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
</a>

Weaveは[Cohere Python library](https://github.com/cohere-ai/cohere-python)を介して行われたLLM呼び出しを`weave.init()`が呼び出された後に自動的に追跡およびログに記録します。

## トレース

LLMアプリケーションのトレースを中央データベースに保存することは、開発中も本番環境でも重要です。これらのトレースはデバッグに使用し、アプリケーションの改善に役立つデータセットとしても活用できます。

Weaveは自動的に[cohere-python](https://github.com/cohere-ai/cohere-python)のトレースをキャプチャします。通常通りライブラリを使用できます。まず`weave.init()`を呼び出します：

```python
import cohere
import os
import weave

# Use the Cohere library as usual
co = cohere.Client(api_key=os.environ["COHERE_API_KEY"])

# highlight-next-line
weave.init("cohere_project")

response = co.chat(
    message="How is the weather in Boston?",
    # perform web search before answering the question. You can also use your own custom connector.
    connectors=[{"id": "web-search"}],
)
print(response.text)
```

cohereモデルの強力な機能の一つは[connectors](https://docs.cohere.com/docs/overview-rag-connectors#using-connectors-to-create-grounded-generations)を使用することで、エンドポイント側で他のAPIにリクエストを行うことができます。レスポンスには、コネクタから返されたドキュメントにリンクする引用要素を含む生成テキストが含まれます。

[![cohere\_trace.png](imgs/cohere_trace.png)](https://wandb.ai/capecape/cohere_dev/weave/calls)

<Note>
  私たちはCohere`Client.chat`、`AsyncClient.chat`、`Client.chat_stream`、および`AsyncClient.chat_stream`メソッドをパッチして、LLM呼び出しを追跡できるようにします。
</Note>

## 独自のopsでラップする

Weave opsは結果を*reproducible*にします。実験中にコードを自動的にバージョン管理し、入力と出力をキャプチャします。単に[`@weave.op()`](/ja/guides/tracking/ops)でデコレートされた関数を作成してCohereのチャットメソッドを呼び出すだけで、Weaveが入力と出力を追跡します。以下は例です：

```python
import cohere
import os
import weave

co = cohere.Client(api_key=os.environ["COHERE_API_KEY"])

weave.init("cohere_project")

# highlight-next-line
@weave.op()
def weather(location: str, model: str) -> str:
    response = co.chat(
        model=model,
        message=f"How is the weather in {location}?",
        # perform web search before answering the question. You can also use your own custom connector.
        connectors=[{"id": "web-search"}],
    )
    return response.text

print(weather("Boston", "command"))
```

[![cohere\_ops.png](imgs/cohere_ops.png)](https://wandb.ai/capecape/cohere_dev/weave/calls)

## `Model`を作成して実験を容易にする

多くの要素が動いている場合、実験の整理は困難です。[`Model`](/ja/guides/core-types/models)クラスを使用することで、システムプロンプトや使用しているモデルなど、アプリの実験的な詳細をキャプチャして整理できます。これにより、アプリの異なるイテレーションを整理して比較するのに役立ちます。

コードのバージョン管理と入力/出力のキャプチャに加えて、[`Model`](/ja/guides/core-types/models)はアプリケーションの動作を制御する構造化されたパラメータをキャプチャし、どのパラメータが最も効果的だったかを簡単に見つけることができます。Weave Modelsは`serve`、および[`Evaluation`](/ja/guides/core-types/evaluations)と一緒に使用することもできます。

以下の例では、`model`と`temperature`で実験できます。これらのいずれかを変更するたびに、新しい*version*の`WeatherModel`が得られます。

```python
import weave
import cohere
import os

weave.init('weather-cohere')

class WeatherModel(weave.Model):
    model: str
    temperature: float
  
    @weave.op()
    def predict(self, location: str) -> str:
        co = cohere.Client(api_key=os.environ["COHERE_API_KEY"])
        response = co.chat(
            message=f"How is the weather in {location}?",
            model=self.model,
            temperature=self.temperature,
            connectors=[{"id": "web-search"}]
        )
        return response.text

weather_model = WeatherModel(
    model="command",
    temperature=0.7
)
result = weather_model.predict("Boston")
print(result)
```

[![cohere\_model.png](imgs/cohere_model.png)](https://wandb.ai/capecape/cohere_dev/weave/models)
