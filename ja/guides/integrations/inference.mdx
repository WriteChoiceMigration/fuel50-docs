# W\&B Inference

*W\&B Inference*W\&B Weaveと OpenAI互換APIを通じて、主要なオープンソース基盤モデルへのアクセスを提供します。W\&B Inferenceを使用すると、以下のことが可能です：

* ホスティングプロバイダーに登録したり、モデルをセルフホスティングしたりすることなく、AIアプリケーションやエージェントを開発できます。
* W\&B Weave Playgroundでサポートされているモデルを試すことができます。

<Warning>
  W\&B Inferenceクレジットは、無料、Pro、およびアカデミックプランに期間限定で含まれています。Enterpriseでは利用可能性が異なる場合があります。クレジットを使い切った後：

  * 無料アカウントは、Inferenceの使用を継続するためにProプランにアップグレードする必要があります。
  * Proプランユーザーは、モデル固有の価格に基づいて、Inferenceの超過分が毎月請求されます。

  詳細については、[価格ページ](https://wandb.ai/site/pricing/)および[W\&B Inferenceモデルコスト](https://wandb.ai/site/pricing/inference)をご覧ください。
</Warning>

Weaveを使用すると、W\&B Inferenceを活用したアプリケーションのトレース、評価、モニタリング、および反復が可能です。

| モデル              | モデルID（API使用時）                             | タイプ       | コンテキストウィンドウ | パラメータ                  | 説明                                                  |
| ---------------- | ----------------------------------------- | --------- | ----------- | ---------------------- | --------------------------------------------------- |
| DeepSeek R1-0528 | deepseek-ai/DeepSeek-R1-0528              | テキスト      | 161K        | 37B - 680B（アクティブ - 合計） | 複雑なコーディング、数学、構造化文書分析など、精密な推論タスク向けに最適化されています。        |
| DeepSeek V3-0324 | deepseek-ai/DeepSeek-V3-0324              | テキスト      | 161K        | 37B - 680B（アクティブ - 合計） | 高複雑性の言語処理と包括的な文書分析向けに調整された堅牢なMixture-of-Expertsモデル。 |
| Llama 3.1 8B     | meta-llama/Llama-3.1-8B-Instruct          | テキスト      | 128K        | 8B（合計）                 | 応答性の高い多言語チャットボット対話向けに最適化された効率的な会話モデル。               |
| Llama 3.3 70B    | meta-llama/Llama-3.3-70B-Instruct         | テキスト      | 128K        | 70B（合計）                | 会話タスク、詳細な指示への対応、コーディングに優れた多言語モデル。                   |
| Llama 4 Scout    | meta-llama/Llama-4-Scout-17B-16E-Instruct | テキスト、ビジョン | 64K         | 17B - 109B（アクティブ - 合計） | テキストと画像理解を統合したマルチモーダルモデルで、視覚タスクと複合分析に最適。            |
| Phi 4 Mini       | microsoft/Phi-4-mini-instruct             | テキスト      | 128K        | 3.8B（アクティブ - 合計）       | リソースが制限された環境での迅速な応答に最適なコンパクトで効率的なモデル。               |

このガイドでは、以下の情報を提供します：

* [前提条件](#prerequisites)
  * [Pythonを介してAPIを使用するための追加前提条件](#additional-prerequisites-for-using-the-api-via-python)
* [API仕様](#api-specification)
  * [エンドポイント](#endpoint)
  * [利用可能なメソッド](#available-methods)
    * [チャット補完](#chat-completions)
    * [サポートされているモデルの一覧](#list-supported-models)
* [使用例](#usage-examples)
* [UI](#ui)
  * [Inferenceサービスへのアクセス](#access-the-inference-service)
  * [Playgroundでモデルを試す](#try-a-model-in-the-playground)
  * [複数のモデルを比較する](#compare-multiple-models)
  * [請求と使用情報を表示する](#view-billing-and-usage-information)
* [使用情報と制限](#usage-information-and-limits)
* [APIエラー](#api-errors)

## 前提条件

APIまたはW\&B Weave UIを介してW\&B Inferenceサービスにアクセスするには、以下の前提条件が必要です。

1. W\&Bアカウント。[ここ](https://app.wandb.ai/login?signup=true&_gl=1*1yze8dp*_ga*ODIxMjU5MTk3LjE3NDk0OTE2NDM.*_ga_GMYDGNGKDT*czE3NDk4NDYxMzgkbzEyJGcwJHQxNzQ5ODQ2MTM4JGo2MCRsMCRoMA..*_ga_JH1SJHJQXJ*czE3NDk4NDU2NTMkbzI1JGcxJHQxNzQ5ODQ2MTQ2JGo0NyRsMCRoMA..*_gcl_au*MTE4ODk1MzY1OC4xNzQ5NDkxNjQzLjk1ODA2MjQwNC4xNzQ5NTgyMTUzLjE3NDk1ODIxNTM.)からサインアップしてください。
2. W\&B APIキー。APIキーは[https://wandb.ai/authorize](https://wandb.ai/authorize)で取得できます。
3. W\&Bプロジェクト。
4. Pythonを介してInferenceサービスを使用している場合は、[Pythonを介してAPIを使用するための追加前提条件](#additional-prerequisites-for-using-the-api-via-python)を参照してください。

### Pythonを介してAPIを使用するための追加前提条件

PythonでInference APIを使用するには、まず一般的な前提条件を完了してから、ローカル環境に`openai`と`weave`ライブラリをインストールします：

```bash
pip install openai weave
```

<Note>
  `weave`ライブラリは、Weaveを使用してLLMアプリケーションをトレースする場合にのみ必要です。Weaveの使用開始については、[Weave クイックスタート](../../quickstart.mdx)を参照してください。

  WeaveでW\&B Inferenceサービスを使用する方法を示す使用例については、[API使用例](#usage-examples)を参照してください。
</Note>

## API仕様

以下のセクションでは、API仕様情報とAPI使用例を提供します。

* [エンドポイント](#endpoint)
* [利用可能なメソッド](#available-methods)
* [使用例](#usage-examples)

### エンドポイント

Inferenceサービスは、以下のエンドポイントを通じてアクセスできます：

```plaintext
https://api.inference.wandb.ai/v1
```

<Warning>
  このエンドポイントにアクセスするには、Inferenceサービスクレジットが割り当てられたW\&Bアカウント、有効なW\&B APIキー、およびW\&Bエンティティ（「チーム」とも呼ばれる）とプロジェクトが必要です。このガイドのコードサンプルでは、エンティティ（チーム）とプロジェクトは`<your-team>\<your-project>`と呼ばれています。
</Warning>

### 利用可能なメソッド

Inferenceサービスは、以下のAPIメソッドをサポートしています：

* [チャット補完](#chat-completions)
* [サポートされているモデルの一覧](#list-supported-models)

#### チャット補完

利用可能な主要なAPIメソッドは`/chat/completions`で、サポートされているモデルにメッセージを送信し、補完を受け取るためのOpenAI互換のリクエスト形式をサポートしています。WeaveでW\&B Inferenceサービスを使用する方法を示す使用例については、[API使用例](#usage-examples)を参照してください。

チャット補完を作成するには、以下が必要です：

* InferenceサービスのベースURL`https://api.inference.wandb.ai/v1`
* W\&B APIキー`<your-api-key>`
* W\&Bエンティティとプロジェクト名`<your-team>/<your-project>`
* 使用したいモデルのID、以下のいずれか：
  * `meta-llama/Llama-3.1-8B-Instruct`
  * `deepseek-ai/DeepSeek-V3-0324`
  * `meta-llama/Llama-3.3-70B-Instruct`
  * `deepseek-ai/DeepSeek-R1-0528`
  * `meta-llama/Llama-4-Scout-17B-16E-Instruct`
  * `microsoft/Phi-4-mini-instruct`

<Tabs>
  <Tab title="Bash">
    ```bash
    curl https://api.inference.wandb.ai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer <your-api-key>" \
      -H "OpenAI-Project: <your-team>/<your-project>" \
      -d '{
        "model": "<model-id>",
        "messages": [
          { "role": "system", "content": "You are a helpful assistant." },
          { "role": "user", "content": "Tell me a joke." }
        ]
      }'
    ```
  </Tab>

  <Tab title="Python">
    ```python
    import openai

    client = openai.OpenAI(
        # The custom base URL points to W&B Inference
        base_url='https://api.inference.wandb.ai/v1',

        # Get your API key from https://wandb.ai/authorize
        # Consider setting it in the environment as OPENAI_API_KEY instead for safety
        api_key="<your-api-key>",

        # Team and project are required for usage tracking
        project="<your-team>/<your-project>",
    )

    # Replace <model-id> with any of the following values:
    # meta-llama/Llama-3.1-8B-Instruct
    # deepseek-ai/DeepSeek-V3-0324
    # meta-llama/Llama-3.3-70B-Instruct
    # deepseek-ai/DeepSeek-R1-0528
    # meta-llama/Llama-4-Scout-17B-16E-Instruct
    # microsoft/Phi-4-mini-instruct

    response = client.chat.completions.create(
        model="<model-id>",
        messages=[
            {"role": "system", "content": "<your-system-prompt>"},
            {"role": "user", "content": "<your-prompt>"}
        ],
    )

    print(response.choices[0].message.content)
    ```
  </Tab>
</Tabs>

#### サポートされているモデルの一覧

APIを使用して、現在利用可能なすべてのモデルとそのIDを照会します。これは、モデルを動的に選択したり、環境で利用可能なものを検査したりするのに役立ちます。

<Tabs>
  <Tab title="Bash">
    ```bash
    curl https://api.inference.wandb.ai/v1/models \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer <your-api-key>" \
      -H "OpenAI-Project: <your-team>/<your-project>" \
    ```
  </Tab>

  <Tab title="Python">
    ```python
    import openai

    client = openai.OpenAI(
        base_url="https://api.inference.wandb.ai/v1",
        api_key="<your-api-key>",
        project="<your-team>/<your-project>"
    )

    response = client.models.list()

    for model in response.data:
        print(model.id)
    ```
  </Tab>
</Tabs>

## 使用例

このセクションでは、W\&B InferenceをWeaveで使用する方法を示すいくつかの例を提供します：

* [基本例：Llama 3.1 8BをWeaveでトレースする](#basic-example-trace-llama-31-8b-with-weave)
* [高度な例：Weave評価とリーダーボードをInferenceサービスで使用する](#advanced-example-use-weave-evaluations-and-leaderboards-with-the-inference-service)

### 基本例：Llama 3.1 8BをWeaveでトレースする

以下のPythonコードサンプルは、**Llama 3.1 8B**モデルにW\&B Inference APIを使用してプロンプトを送信し、Weaveで呼び出しをトレースする方法を示しています。トレースを使用すると、LLM呼び出しの完全な入出力をキャプチャし、パフォーマンスを監視し、Weave UIで結果を分析できます。

<Tip>
  詳細については[tracing in Weave](../tracking/tracing.mdx)をご覧ください。
</Tip>

この例では：

* あなたは`@weave.op()`-デコレートされた関数`run_chat`を定義し、OpenAI互換クライアントを使用してチャット完了リクエストを行います。
* トレースは記録され、W\&Bエンティティとプロジェクトに関連付けられます`project="<your-team>/<your-project>`
* この関数はWeaveによって自動的にトレースされるため、その入力、出力、レイテンシー、およびメタデータ（モデルIDなど）が記録されます。
* 結果はターミナルに表示され、トレースは**Traces**タブの[https://wandb.ai](https://wandb.ai)の指定されたプロジェクトの下に表示されます。

この例を使用するには、[general prerequisites](#prerequisites)と[Additional prerequisites for using the API via Python](#additional-prerequisites-for-using-the-api-via-python)を完了する必要があります。

```python
import weave
import openai

# Set the Weave team and project for tracing
weave.init("<your-team>/<your-project>")

client = openai.OpenAI(
    base_url='https://api.inference.wandb.ai/v1',

    # Get your API key from https://wandb.ai/authorize
    api_key="<your-api-key>",

    # Required for W&B inference usage tracking
    project="wandb/inference-demo",
)

# Trace the model call in Weave
@weave.op()
def run_chat():
    response = client.chat.completions.create(
        model="meta-llama/Llama-3.1-8B-Instruct",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Tell me a joke."}
        ],
    )
    return response.choices[0].message.content

# Run and log the traced call
output = run_chat()
print(output)
```

コードサンプルを実行すると、ターミナルに表示されるリンク（例：`https://wandb.ai/<your-team>/<your-project>/r/call/01977f8f-839d-7dda-b0c2-27292ef0e04g`）をクリックするか、以下の手順でWeaveでトレースを表示できます：

1. ナビゲートして[https://wandb.ai](https://wandb.ai)。
2. 選択**Traces**タブでWeaveトレースを表示します。

次に、[advanced example](#advanced-example-use-weave-evaluations-and-leaderboards-with-the-inference-service)を試してみてください。

![Traces display](imgs/image.png)

### 高度な例：Weave EvaluationsとLeaderboardsをインファレンスサービスで使用する

インファレンスサービスでWeaveを使用して[trace model calls](../tracking/tracing.mdx)するだけでなく、[evaluate performance](../core-types/evaluations.mdx)、および[publish a leaderboard](../core-types/leaderboards.mdx)することもできます。次のPythonコードサンプルは、シンプルな質問と回答のデータセットで2つのモデルを比較します。

この例を使用するには、[general prerequisites](#prerequisites)と[Additional prerequisites for using the API via Python](#additional-prerequisites-for-using-the-api-via-python)を完了する必要があります。

```python
import os
import asyncio
import openai
import weave
from weave.flow import leaderboard
from weave.trace.ref_util import get_ref

# Set the Weave team and project for tracing
weave.init("<your-team>/<your-project>")

dataset = [
    {"input": "What is 2 + 2?", "target": "4"},
    {"input": "Name a primary color.", "target": "red"},
]

@weave.op
def exact_match(target: str, output: str) -> float:
    return float(target.strip().lower() == output.strip().lower())

class WBInferenceModel(weave.Model):
    model: str

    @weave.op
    def predict(self, prompt: str) -> str:
        client = openai.OpenAI(
            base_url="https://api.inference.wandb.ai/v1",
            # Get your API key from https://wandb.ai/authorize
            api_key="<your-api-key>",
            # Required for W&B inference usage tracking
            project="<your-team>/<your-project>",
        )
        resp = client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
        )
        return resp.choices[0].message.content

llama = WBInferenceModel(model="meta-llama/Llama-3.1-8B-Instruct")
deepseek = WBInferenceModel(model="deepseek-ai/DeepSeek-V3-0324")

def preprocess_model_input(example):
    return {"prompt": example["input"]}

evaluation = weave.Evaluation(
    name="QA",
    dataset=dataset,
    scorers=[exact_match],
    preprocess_model_input=preprocess_model_input,
)

async def run_eval():
    await evaluation.evaluate(llama)
    await evaluation.evaluate(deepseek)

asyncio.run(run_eval())

spec = leaderboard.Leaderboard(
    name="Inference Leaderboard",
    description="Compare models on a QA dataset",
    columns=[
        leaderboard.LeaderboardColumn(
            evaluation_object_ref=get_ref(evaluation).uri(),
            scorer_name="exact_match",
            summary_metric_path="mean",
        )
    ],
)

weave.publish(spec)
```

次のコードサンプルを実行した後、W\&Bアカウントに移動します[https://wandb.ai/](https://wandb.ai/) and:

* ナビゲートして**Traces**タブで[view your traces](../tracking/tracing.mdx)
* ナビゲートして**Evals**タブで[view your model evaluations](../core-types/evaluations.mdx)
* ナビゲートして**Leaders**タブで[view the generated leaderboard](../core-types/leaderboards.mdx)

![View your model evaluations](imgs/inference-advanced-evals.png)

![View your traces](imgs/inference-advanced-leaderboard.png)

## UI

次のセクションでは、W\&B UIからインファレンスサービスを使用する方法について説明します。UIを介してインファレンスサービスにアクセスする前に、[prerequisites](#prerequisites)を完了してください。

### インファレンスサービスへのアクセス

Weave UIから2つの異なる場所でインファレンスサービスにアクセスできます：

* [直接リンク](#direct-link)
* [Inferenceタブから](#from-the-inference-tab)
* [Playgroundタブから](#from-the-playground-tab)

#### 直接リンク

ナビゲートして[https://wandb.ai/inference](https://wandb.ai/inference)。

#### Inferenceタブから

1. W\&Bアカウントに移動します[https://wandb.ai/](https://wandb.ai/)。
2. 左側のサイドバーから**Inference**を選択します。利用可能なモデルとモデル情報が表示されるページが表示されます。

![The Inference tab](imgs/inference-ui.png)

#### Playgroundタブから

1. 左側のサイドバーから**Playground**を選択します。Playgroundチャットインターフェースが表示されます。
2. LLMドロップダウンリストから**W\&B Inference**にマウスオーバーします。利用可能なW\&B Inferenceモデルのドロップダウンが右側に表示されます。
3. W\&B Inferenceモデルのドロップダウンから、以下のことができます：
   * 利用可能な任意のモデルの名前をクリックして[try it in the Playground](#try-a-model-in-the-playground)。
   * [Playgroundで1つ以上のモデルを比較する](#compare-multiple-models)

![The Inference models dropdown in Playground](imgs/inference-playground.png)

### Playgroundでモデルを試す

一度[selected a model using one of the access options](#access-the-inference-service)したら、Playgroundでモデルを試すことができます。以下のアクションが利用可能です：

* [モデル設定とパラメータをカスタマイズする](../tools/playground.md#customize-settings)
* [メッセージの追加、再試行、編集、削除](../tools/playground.md#message-controls)
* [カスタム設定でモデルを保存して再利用する](../tools/playground.md#saved-models)
* [複数のモデルを比較する](#compare-multiple-models)

![Using an Inference model in the Playground](imgs/inference-playground-single.png)

### 複数のモデルを比較する

Playgroundで複数のInferenceモデルを比較できます。比較ビューには2つの異なる場所からアクセスできます：

* [Inferenceタブから比較ビューにアクセスする](#access-the-compare-view-from-the-inference-tab)
* [Playgroundタブから比較ビューにアクセスする](#access-the-compare-view-from-the-playground-tab)

#### Inferenceタブから比較ビューにアクセスする

1. 左側のサイドバーから**Inference**を選択します。利用可能なモデルとモデル情報が表示されるページが表示されます。
2. 比較するモデルを選択するには、モデルカードの任意の場所（モデル名を除く）をクリックします。選択を示すためにモデルカードの境界線が青色でハイライト表示されます。
3. 比較したい各モデルについてステップ2を繰り返します。
4. 選択したカードのいずれかで、**Compare N models in the Playground**ボタン（`N`は比較しているモデルの数です。例えば、3つのモデルが選択されている場合、ボタンには**Compare 3 models in the Playground**と表示されます）をクリックします。比較ビューが開きます。

これで、Playgroundでモデルを比較し、[Try a model in the Playground](#try-a-model-in-the-playground)で説明されている機能を使用できます。

![Select multiple models to compare in Playground](imgs/inference-playground-compare.png)

#### Playgroundタブから比較ビューにアクセスする

1. 左側のサイドバーから**Playground**を選択します。Playgroundチャットインターフェースが表示されます。
2. LLMドロップダウンリストから**W\&B Inference**にマウスオーバーします。利用可能なW\&B Inferenceモデルのドロップダウンが右側に表示されます。
3. ドロップダウンから**Compare**を選択します。**Inference**タブが表示されます。
4. 比較するモデルを選択するには、モデルカードの任意の場所（モデル名を除く）をクリックします。選択を示すためにモデルカードの境界線が青色でハイライト表示されます。
5. 比較したい各モデルについてステップ4を繰り返します。
6. 選択したカードのいずれかで、**Compare N models in the Playground** button (`N` は比較しているモデルの数です。例えば、3つのモデルが選択されている場合、ボタンは **Compare 3 models in the Playground**)と表示されます。比較ビューが開きます。

これで、Playgroundでモデルを比較し、[Try a model in the Playground](#try-a-model-in-the-playground)で説明されている機能を使用できます。

### 請求と使用情報を表示する

組織の管理者は、W\&B UIから直接、現在のInferenceクレジット残高、使用履歴、および今後の請求（該当する場合）を追跡できます：

1. W\&B UIで、W\&Bの**Billing**ページに移動します。
2. 右下隅に、Inference請求情報カードが表示されます。ここから以下のことができます：

* Inference請求情報カードの**View usage**ボタンをクリックして、時間の経過に伴う使用状況を確認します。
* 有料プランを利用している場合は、今後のinference料金を確認できます。

<Tip>
  [Inference pricing page for a breakdown of per-model pricing](https://wandb.ai/site/pricing/inference)
</Tip>

## 使用情報と制限

以下のセクションでは、重要な使用情報と制限について説明します。サービスを使用する前に、この情報をよく理解しておいてください。

### 地理的制限

Inferenceサービスは、サポートされている地理的位置からのみアクセスできます。詳細については、[Terms of Service](https://docs.coreweave.com/docs/policies/terms-of-service/terms-of-use#geographic-restrictions)をご覧ください。

### 同時実行制限

公平な使用と安定したパフォーマンスを確保するために、W\&B Inference APIはユーザーとプロジェクトレベルでレート制限を適用しています。これらの制限は以下に役立ちます：

* 誤用を防止しAPIの安定性を保護する
* すべてのユーザーのアクセスを確保する
* インフラストラクチャの負荷を効果的に管理する

レート制限を超えると、APIは`429 Concurrency limit reached for requests`レスポンスを返します。このエラーを解決するには、同時リクエスト数を減らしてください。

### 価格

モデルの価格情報については、[https://wandb.ai/site/pricing/inference](https://wandb.ai/site/pricing/inference)をご覧ください。

## APIエラー

| エラーコード | メッセージ                                                                       | 原因                                         | 解決策                                                       |
| ------ | --------------------------------------------------------------------------- | ------------------------------------------ | --------------------------------------------------------- |
| 401    | Invalid Authentication                                                      | 認証情報が無効であるか、W\&Bプロジェクトのエンティティや名前が正しくありません。 | 正しいAPIキーが使用されていることを確認し、W\&Bプロジェクト名とエンティティが正しいことを確認してください。 |
| 403    | Country, region, or territory not supported                                 | サポートされていない場所からAPIにアクセスしています。               | [Geographic restrictions](#geographic-restrictions)       |
| 429    | Concurrency limit reached for requests                                      | 同時リクエストが多すぎます。                             | 同時リクエスト数を減らしてください。                                        |
| 429    | You exceeded your current quota, please check your plan and billing details | クレジットがなくなったか、月間支出上限に達しました。                 | より多くのクレジットを購入するか、制限を引き上げてください。                            |
| 500    | The server had an error while processing your request                       | 内部サーバーエラー。                                 | 少し待ってから再試行し、問題が解決しない場合はサポートに連絡してください。                     |
| 503    | The engine is currently overloaded, please try again later                  | サーバーが高いトラフィックを経験しています。                     | 少し時間をおいてからリクエストを再試行してください。                                |
